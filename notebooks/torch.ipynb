{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage:\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhi = pd.read_csv('uhi_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = uhi.drop(columns=['Latitude', 'Longitude', 'UHI Index', 'datetime'], axis=1)\n",
    "y = uhi['UHI Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled\n",
    ", dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "class UHIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "# train = UHIDataset(X_tensor, y_tensor)\n",
    "# train_loader = DataLoader(train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            ResidualBlock(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            ResidualBlock(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            ResidualBlock(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            ResidualBlock(64),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = Model(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/10\n",
      "Epoch 1/300, Loss: 0.8999317273308959, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8417412118066715, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8219349520115913, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.7988628767713716, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7852990400942066, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7811678399013567, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7670513580117044, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.753342525113987, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7478642810749102, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7414290806915187, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7331109537353998, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7319290358808976, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7214471116850648, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.706886047803903, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7066392532632321, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7050109441521801, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6847045798844928, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.672918289899826, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6655643555936934, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6582582996615881, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6674467943891694, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6453989962988262, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6453279792507992, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6386933866180952, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6228357368632208, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6234560680540302, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6025052481814276, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6041410240945937, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.6014808080618894, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5824353046055082, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5699264331708981, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.565696111962765, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5703393703774561, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5573509595816648, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5535167448128326, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5409913097001329, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5245729880996898, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5236550992048239, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.520255946283099, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5017320611808873, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5105274512043482, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.49062876761714114, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4945992061608954, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4866951881330224, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.47126459773582746, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4552477949027774, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.4594792746290376, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.4508642644821843, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.4375044016898433, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.4373724030542977, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.4295147725298435, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.4197775019875056, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.4222858974450751, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.41070710631865487, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.4050508737564087, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.38960283579705635, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.38576542388034774, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.37817343579062934, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.3817237485435945, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3606176685683335, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.36022358453726466, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.35051591845252844, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.35630454330504696, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.3441312113140203, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.32757992118219786, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.32868078596229794, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.32319123099876357, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.31290659610229205, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3045573443928851, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.3100076774253121, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.29806335515613797, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.29596516659742667, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.2843096859847443, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.27592003326627274, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2771036496645288, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.26671556430526927, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.26099399599847917, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2677115064255799, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.2573353844352915, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2591638702757751, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.24008715944954112, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.2341901426073871, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.23533683581442771, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.2396846077864683, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.23363935230653496, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.2136171866632715, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.21718802048435695, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.21390136378475383, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.20473260875744156, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.19996103606646573, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.20022614139922057, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.18789723242008233, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.19008688524931291, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.1915272868509534, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.19840158446680142, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.17936455684749386, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.18182882308205472, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.17832031581975236, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.17099085478465767, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.16594673193331005, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.1633276246205161, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.16960630282948289, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.16250653895018977, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.16376291404042062, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.14689389653975451, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.14486370528046089, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.14542824114802516, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.1392209925040414, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.14786257340183742, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.14130646801447566, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.13171618610997743, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.12980708875988103, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.12566670652809023, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.12664845765014238, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13053614740507513, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.11943800888861282, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.1194731254555002, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.12351031035562104, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.11847332091648367, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11628693700591221, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.11209649771829194, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.1145642837391624, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.11396523763107348, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.10784632533411437, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.10323505443108233, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.11280751690457139, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.10708957957693294, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.10678268356036537, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10375144059144997, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.09313608102406128, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.09311663991288294, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.09537514030367514, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.09108601585973668, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.08753866475971439, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09490395445801035, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.09159803645143026, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.0930242964738532, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.09199474813251556, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.08993000631468205, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08456674609569055, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.08668680705978901, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08489326048123685, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.07860144185303133, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08713269181832482, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.08434468158815481, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.07845140408866014, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.0764409786066677, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.07575887295452854, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07511211912843245, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07597820418356341, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.0741995454683334, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.0690275108606755, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07500249746290943, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.06994520071186597, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.07094382975674883, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.06656923512869244, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.06789875238002101, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.066512293453458, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06160829749099816, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06597597562248193, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06356590986251831, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.06056064202249804, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06652925003178511, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06344687707627876, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06514967888405052, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.0626088670820375, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.05947561642225785, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.06032065188960184, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.05876760297938238, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.059031194665386706, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.056982114062279085, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.05771286266891262, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.05721637703289714, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.05752687861176231, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05506185850104953, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.0527955308745179, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05317091286371026, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.053791882446672344, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05614814063227629, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.05303619141820111, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.05319356427916998, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.05132989689141889, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.04922871713679802, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.048799627590217165, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.04873419128641297, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.047884345384715477, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04787752110086664, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.04932965206194528, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04804936249422122, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.045901258700067483, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.045940539457752734, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04752844506049458, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.04640663342102419, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.045163195674555213, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04331750785814056, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.044040705207027964, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.044553388216638866, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04269707118031345, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.042799562663783, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04224362481338314, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04146334333227405, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.043246155180319955, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.03811602454773987, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04060634249184705, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.03990345778344553, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.03921217940559116, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.03873294575399236, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.036816532098794284, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.036859533761309675, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.03757157228604148, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.03940406337946276, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.037115477001931095, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.036908869171821616, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.0371118634467638, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.037896505663100676, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03607418225441553, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.03708265561468994, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03759519296073461, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03526017939836919, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03340404940462565, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03489584360224537, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.03451399696118469, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03536508817084228, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03438701879091655, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03390737108980553, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03300252438911909, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.033322450905283796, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.033126533644486075, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.032196418606216394, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.033298851356287545, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.03171469577694241, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.03032172681127168, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.030750044704049448, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.030761542718244505, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.029306346527006054, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.03009311489383631, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.03145862196253825, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.03158549806456777, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.030082567158756377, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.029629247569585147, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.029678702118653286, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.028739220781039587, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.02944026491310023, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.03052785808715639, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.028915919504965408, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.029264025508037098, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.029091105265896533, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.028797095877271663, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.028285979375809053, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.028256117566665517, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.027709709197471413, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.029261568160373952, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.028247953592976438, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.027861677133772945, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.027468418843949897, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.027048017122323, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.02774931515177971, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.02596117991105288, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.02575442569825468, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.026264635044374044, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.027100924001652982, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.025833174869229522, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.027043247477540486, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.026003622261312188, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.026123575984111316, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.025891762368286712, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.025838245366570315, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.026295418556355223, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.026808229546192327, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.026355268858090232, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.02671426798723921, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.02595862755669823, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.02536821810976614, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.025977245720598518, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.026533161561134496, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.02535931609240891, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02538238652050495, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.025640852537147606, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.025250349901145018, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.02640992148390299, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.0251508231121528, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.025286391731115836, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.025620117076212846, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.023974927130473566, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.025734823892671097, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.02440371008330508, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.02596021429458751, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.024837947745300546, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.025338025952253162, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.024608953808776184, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.025803677549090565, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.025603542788119257, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.025556763775552375, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.024269064463957955, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.024294749138098728, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.02412126642427867, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.025487693604317647, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.025185126940943773, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.025597312617339666, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.024986420581235163, Learning Rate: 1e-06\n",
      "Fold 1 R2: 0.602157711982727\n",
      "\n",
      "Fold 2/10\n",
      "Epoch 1/300, Loss: 0.9078251991090895, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8410285967814771, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8205815870550615, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8056175821944128, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7884288369854794, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7820842017101336, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7688247780256634, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7644374838358239, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7460779260985458, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7450425421135335, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7312073134168794, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7270099392420128, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7207455129563054, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7181184752077996, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7024466855616509, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7042101757435859, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6889466186867484, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6828099866456623, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6633038837698442, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6698594274400156, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6534002355382412, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.646843987175181, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6505270622953584, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6481471650208099, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.635177991058253, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6251064029675496, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6326352889779248, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6243675173837927, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.6110419551782971, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.6000949008555352, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5923298525659344, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5949404775341854, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5860051701340494, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.565039521153969, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5611799185789084, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5536856651306152, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5606314570088929, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5354319950447807, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5292171224763121, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.533760707966889, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5270745644841013, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.5113089658037017, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.5137278562105154, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.5065626064433327, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.4918164807029917, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4863350281987009, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.48383319566521465, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.46860200619395775, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.4610978364944458, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.46995806354510633, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.44158536686172967, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.44368439139444615, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.44323583154738705, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.4299446668805955, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.4207135159758073, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.424947918593129, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.41330675988257687, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3955216577535943, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.39789490345158157, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3834697819208797, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.3836850840834123, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.3705199922941908, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.37385756399812575, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.3665086857125729, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.3582193274286729, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.3476715114297746, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.3473384495400175, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.3440136087091663, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3327480482904217, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.3358507678855824, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.3251344184332256, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.32410470990440515, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.32215735165378717, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.2919436789388898, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2905664817441868, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.2960837458885169, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.2880706498517266, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.27136455080177213, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.28303369313855714, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.27163047937652735, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.2687145488548882, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.2684010212557225, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.2546633542338504, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.24219945488096792, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2412161981757683, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.23480008608555492, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.23380467257922208, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.22530552895763253, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.23069912591312505, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.215216411254074, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.2109507099737095, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.20584837488735777, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.21197318596930442, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.1958239619867711, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.19538548909410647, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.19575967498217958, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.19433910390244255, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.17820626148317434, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.17834830529327633, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.1719242073123968, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.17844585829143284, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.16642793975298917, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.1655883010995539, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.1728394264473191, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.15655017031144491, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.1494396973071219, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.1548435748377933, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.15912080754207658, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.1473560929298401, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.14344877212107937, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.14272234890657134, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.13549732530041586, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.1371484435623205, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.1339537561694278, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13193715250567545, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.13025780154179922, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.13273764382812042, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.12898146672339378, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.13019410511360893, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.12475265223014204, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.12374740980471237, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.11760221214234075, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.1106431883158563, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.10460583375224584, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.10412741348713259, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.11181135980200164, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.1116402190697344, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.10501246173170549, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10245363404856453, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.10466560410170615, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.10613538023037246, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.10094170913666109, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.10000962438651279, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09109896614770346, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09190273553698877, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.0935587676454194, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.09312510009430632, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.0931008057786694, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.09207800841784175, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08677919458927988, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.08951263695578032, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.09221856343218043, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08533687092646768, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08214158960912801, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.0776571797419198, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.07896695125706588, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.08207934264895282, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.07646025311720522, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07838054279549213, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07569450206017192, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.07711569653658927, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.07294101018128515, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07108779652397844, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.07086161707964124, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.06933196841538707, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.065078562526386, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.06663584393225139, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.06527906604394128, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06851972182151637, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.07062977010124846, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06306453751800936, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.0610850616535054, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06318744960465009, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06380439648711228, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06685487610063975, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06491058027442498, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.060376322910755495, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.062101587275915505, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.061836596578359604, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.059930290649586083, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.05760773615557936, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.05729572413654267, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.05701159086974361, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.05585156513165824, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05913124473977693, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05507239864408216, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05594424408259271, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.052156441290921805, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05096987229359301, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.050296699604656124, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.053085829048782965, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.052693504604357705, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.04890117407599582, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.04785775860087781, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.048637436302025105, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.0497027615769, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04821468566697609, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.04685695596699473, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04726005869009827, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.04570433768573441, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.047680883562263056, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04452403740886646, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.046697186023185525, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.046884436396103873, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.044036372293588484, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04241246576833574, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.0461916969971189, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04325060897707185, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04039748340750797, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.041762882607835755, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.041909511495806, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.04108864509889597, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.040866975684331945, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04143250483689429, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.042702040576104876, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.041510363404132146, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.03849025709625287, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.03804280809303628, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.03834156989108158, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.03899372682635543, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.036437420669613, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03745066244862502, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.03702971169465705, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03660371300729015, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.03610991877562637, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03600953988542285, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.03580142043625252, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03370341583143307, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03372520222505437, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03459073301357559, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03570947168937212, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.03357133844607993, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.034378766777771935, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03337441580487958, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03353958501468731, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.032681475098751765, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03305744617894481, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.032018174075439006, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.03248270631685287, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.03378105297967603, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.032149678756353224, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.030513095492614977, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.0305830730903375, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.030736365744584722, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.031330700562913205, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.03162913694034649, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.029444778196607964, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.029240100750628904, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.029091864069805868, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.030552506965549685, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.029394839312645454, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.029189227431823936, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.028774413527755796, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.030051429461263404, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.028893072676809527, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.029540432145512555, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.028046059669761716, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.028867277352115774, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.02785317628066751, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.026672611626077303, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.028112767877269396, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.027874087278224245, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.02684958465397358, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.026822433276455615, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.027554389443012733, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.025816584972641134, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.02771358985359533, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.02650363658425174, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.02632467792947081, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.027137380447002906, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.026158584328957752, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.02785336087115958, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.02767886690606799, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.02521165983774994, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.02650983430020794, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.025065889299104485, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.026131772447990465, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.02594731972093069, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.0273297221364477, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.0266493764481967, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.025555481686245038, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.024577712333655057, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.026588341598457927, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.026091843560526642, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.02559981406725283, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.0261981893047879, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02583075637775886, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.024623018147164506, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.024682701895414275, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.025359468180922013, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.024797065867276133, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.025305543212762363, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.024535172512825533, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.024531231627245492, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.023606138020942483, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.02533120336600497, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.024070553953134562, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.024274172638601894, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.023568406854343565, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02450525754756188, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.025457898084121414, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.024875462208461912, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.024135265546508983, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.024353602146611936, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.02438122642379773, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.024324588088578063, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.024942490496212923, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.02424711917963209, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.02504437168046266, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.025317701263518275, Learning Rate: 1e-06\n",
      "Fold 2 R2: 0.6417165994644165\n",
      "\n",
      "Fold 3/10\n",
      "Epoch 1/300, Loss: 0.9105636620823341, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8522557088091404, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8174869440778901, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.807032585144043, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.796785342542431, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7884246148640597, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7701140747794623, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7652968515323687, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7568228886097292, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7456996139091782, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7320410465892357, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7131753869449036, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7159866622731655, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7088258953034123, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.701423717450492, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.6859975268569174, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6816392156142222, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6750629023660587, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6613795908191537, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6595538798012311, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6499501123458524, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6444339163695709, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6458979177324078, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6301974612700788, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6151527219180819, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6008081632324412, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.5975294026392924, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.5930414580846135, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.5852941980090323, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5828827608235275, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5679436903211135, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5671663925617556, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5624397095245651, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5502309723745419, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.541973240013364, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5332690681083293, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.534140358620052, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5170867416677596, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5157384363156331, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.498497118678274, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5037945146802105, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.4968703441982028, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4785633747336231, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4728823530523083, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.4667288781721381, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4716982965982413, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.46467157481591914, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.4494490763054618, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.43834641575813293, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.433643191298352, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.44037769150130357, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.42157335824604275, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.4266458493244799, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.4254739027989062, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.42320042924035955, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.3992465730709366, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.3861616023733646, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3907252485993542, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.387616283908675, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3649753633933731, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.3592850081151045, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.36413580235801163, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3664818260865875, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.3499717940635319, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.3466751801816723, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.33960452713543854, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.33394457871400857, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.32477082576178296, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3111536881968945, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.30623489429679096, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.2986661461712439, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.29475572007366374, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.29065816632554503, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.28693806861020343, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.28486257810381393, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.27592057932781267, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.2673126461007927, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.26725278134587444, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.26098347898525526, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2567750683313684, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.2515378211495243, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.2375543945575062, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.23812838955016075, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.2259045997752419, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.23647228637828102, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.23150334063964553, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.2218239147074615, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.21851624039155018, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.21221889781801007, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.21204270177249668, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.20265975764280633, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.19283091088261786, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.1869197234134131, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.198185503105574, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.18481946123551718, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.19151421438289595, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.18070079772909986, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.17380856996095634, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.16875910004482994, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.1752408512785465, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.16177200733483593, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.1620606165143508, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.16366456920587563, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.15437518418589724, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.15092435545181926, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.15158700245090678, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.1548217535018921, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.1424591653520548, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.14351710045262228, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.13556647602515884, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.13461733590575714, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.13354431309654743, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.12639099510410165, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.12583500448661514, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.12988660644881334, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.12943748158367374, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.12672090709586686, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.12720207719108725, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.11698780105083803, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11780864207804957, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.11616780476856835, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.11316776280350323, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.11764145821710176, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.11416001061472712, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.11328897304550002, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.11304254782728002, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.1066876570635204, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.10811223947926413, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10029158458302292, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.10058362593379201, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.10421483284687694, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.09648950511141668, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.09656022724848759, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09802518568084209, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09194606467138362, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.08586180851429323, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.09021035430929329, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.09221396996062013, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.08885280674771417, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08473562199292303, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.08038771270385271, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08441572532623628, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08016761835617355, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.0819694951553888, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.08516412765919408, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.07445033663247205, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.07978535467122175, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.08196099861701832, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.08063546502137486, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07727726190527782, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.07778260041075417, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.06935021196362338, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.06861882732261586, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.06887366023810604, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.07337050007868416, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.07343359753678116, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.0699335843136039, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.06671334175935274, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06626069309967983, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06489406507226485, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.0665325103209743, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.06447582772072358, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06456800210702268, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06225619785770585, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06238445997992648, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.0625935692953158, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06265513320701031, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.06374911784748488, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.061136555256722847, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.06332881809978545, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.06024572140053858, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.062454982460299624, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.055506050044411344, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.05785289435069772, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.058987201061807104, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05377646646450592, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.0543839524063883, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.054933491292633585, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05313462273606771, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.052238853671882725, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.04865513628796686, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.0533735918564887, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.04920558487595637, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.048818371341198305, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.04932459000545212, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.046689935853775545, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04883024122424518, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.048206629160838795, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04592373047637034, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.047367369072346746, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.04439169196765634, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04435337007140057, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.044104382018499734, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.04311072948989989, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04468550672165201, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04383525304213355, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04193692673233491, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.044982239816196354, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04521257486901706, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04076727979545352, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04237871753829944, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.03999234641654582, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04153396226937258, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04077806012539924, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.03936659577715246, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.04198169981754279, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.04069125369379792, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.040993747030255163, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.037604836369805696, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.03865748509481738, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.0364295472685672, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03713849300070654, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.038335262691672846, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03745620287483251, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.03765946379096448, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03647537873704222, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.036996652996992764, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03527469767988482, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.035133442405281184, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03695479568235482, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.034787278787433346, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.034056810591417026, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.034223542041793655, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.033869403072550326, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.033924386919110636, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03216058103061175, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.032465866896547846, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.03231876616991019, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.034516510754069196, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.03260059169010271, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.031990557083789305, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.03189970798130277, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.032109129938143716, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.03271298237805125, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.03125267565439019, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.029709849341572087, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.03133762851829016, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.03031083847148509, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.030417309012971346, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.028941495037531552, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.029989051526483103, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.02981464319591281, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.030332497545058214, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.03127472057844265, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.029353973113849192, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.02870089336758173, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.028186515940329695, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.02788377727700185, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.027986190175708338, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.02792048774942567, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.02832930476133582, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.02825105173772649, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.028134908338513554, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.028499030189800868, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.028139485211312015, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.026999119834243496, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.02662964102729589, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.027985159944318518, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.02658234267861028, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.02680965317295322, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.027561810551376284, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.0277498058408876, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.026438697205880022, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.02667900501408532, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.026440970811851417, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.027323469189526158, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.026221594549243964, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.025001860707055165, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.026608063615387, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.02696921508995038, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.0276388252224726, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.025772693944222564, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.025267777092094663, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.02535312438878832, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.0249165424935614, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.025541580058162726, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02552546744671049, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.025166350063172322, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.025305452556172503, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.025721696877404103, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.024618877451631088, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.02487296153661571, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.025623095044984095, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.02557508716996335, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.02483365466653169, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.02476260870034936, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.02612140970422497, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.024480224009367484, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.025192407419598554, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.024868085838948624, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.024545790816221057, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.024398242264891727, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.025797965983518318, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.025273213799618468, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.025139339857652217, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.024746407012018978, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.025735074017621293, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.02542986721979289, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.02571276570611362, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.02556533141273864, Learning Rate: 1e-06\n",
      "Fold 3 R2: 0.5889679193496704\n",
      "\n",
      "Fold 4/10\n",
      "Epoch 1/300, Loss: 0.901560123962692, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8392685002918485, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8186181572419179, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8056228892712654, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.789394654050658, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7831505914277668, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7724061630949189, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7605360052253627, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7488743979719621, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7390820550013192, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7391349356385726, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7268783133241195, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7226671138896218, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7166170440142667, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7067039971110187, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.6943527165847488, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.685968425077728, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.677990648565413, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6768175786054587, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6650050999243048, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6564154798471475, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6637983548490307, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6568621687496765, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6422284781178341, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6341612237163737, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6340863101090057, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6230508164514469, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.619028352861163, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.607185896816133, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5949898192399665, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.593026710084722, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.590220605270772, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.58444083642356, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5711693461937241, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5599717003635213, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5647831291337556, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5523184024834935, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.538376782513872, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5353041506266292, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5277452649949472, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5222185661521139, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.5206366924545432, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.5067743474169623, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4955630607997315, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.5032055574127391, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4839839633507065, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.5059934711154503, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.4622929152808612, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.4860789998422695, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.46020378643953347, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.4664662657659265, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.45481330156326294, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.43494256538680837, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.4437167885937268, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.43387761825247656, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.42852982878685, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.4151628106078015, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.4020194215110586, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.3926835588262051, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3968920494559445, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.3981894322588474, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.3905804749531082, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3871777118006839, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.36942040618461897, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.36781222816509535, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.3664675268191325, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.35169902689094784, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.3403801827491084, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.34226374912865554, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.33506439494181284, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.3240314880503884, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.3195613782994355, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.3200556251067149, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.3156872950022734, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.3010465838486635, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.2985786733370793, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.29407892442202266, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2979434091079084, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.28512058152428155, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.27518096056920066, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.27297860798956475, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.2704780069710333, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.27367583294458026, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.25323020638544347, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.24781624840784677, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.24421928672096396, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.2389782933117468, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.23710613443127163, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.23954751106757152, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.24317231623432303, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.2303777567193478, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.22183953753755062, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.21338429273683815, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.21603106358383276, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.20542120009283477, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.20080269043204152, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.1955677127536339, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.19673009150767629, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.18657828028066248, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.19335806049123594, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.17698140093420126, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.1955583421867105, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.17970398162739187, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.17395005441164668, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.16995699533933326, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.177012240867826, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.17748001085806497, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.17148056164195266, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.16699057732579076, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.16393277407446993, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.15251007100826577, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.14674154329526273, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.14637436133019532, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.15363346483511262, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.14524555819321283, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.1343867685410041, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.1429769131767599, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.13803734800106363, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.13874902378154708, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.12609831385220152, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.12739795484120334, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.12247524136983895, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.12294058388547052, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.12602187844016885, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.12634966475299642, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.12256565954111799, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.11768384824825238, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.11970205803083468, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.12343405705841282, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.11677167706097229, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.12057607517212252, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.11246835845935194, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.10584947636610345, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.10739886440053771, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.10665160559023483, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.10301794441817683, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.10283975689848766, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.09935217294134671, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.09889634934407246, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.09914818028860455, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.09492238238453865, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.09609445024140273, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.09436203812873817, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.09339636644419236, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.0922765261198901, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.08664011351669891, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.08605446114758902, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.08499238583483273, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.08143033841742744, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.0832998850206031, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.0822313365679753, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.0859824432225167, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.08004611876757839, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.08062108884317966, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.07919289416904692, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.07585963781310033, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.07301426664749279, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.07514126476230501, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.07386691674967355, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.07519202721835691, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.07080896618434146, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.07029275959239731, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06923755197019517, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.07357088160477107, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06879506873179085, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.0689294328323648, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06696368244629872, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.0655576791850072, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.06598895745752734, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.06761544235522234, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.06872122885682914, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.06497595874191839, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.062372687968272195, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.06529430587646327, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.06342459531335891, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05852481416320499, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05998189157888859, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.05994099774692632, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.0569667531342446, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.0565059690535823, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.05518216484143764, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.05709068844967251, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.05714286109314689, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05520096013346051, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.055841763562793974, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.05487109041666683, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.057758462957189054, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.05212834732064718, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.054258891958979115, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.05121884777953353, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.0496480581409569, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.05233669592232644, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.05254303430548952, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.050496217476416236, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04919374451229844, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04928803656108772, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.045246535629222664, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04610445883266533, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04594954952031751, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04675260921822318, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.049801713067896755, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.045135627064523814, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04733328556618358, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04375193650020829, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.04557393623303763, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.043848704876779, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.04459958473998535, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.04349129533843149, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.04477351085766207, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.0427297247172911, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.042837936030347135, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.0417313201565154, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.040958844455359855, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03977696484402765, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.042036136942385116, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.04066644517021089, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.04192242023971262, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.04253237340835076, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03903240418132348, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.04126291116110132, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.038462694875801666, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.04037768042445937, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.038368957470866695, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03739002311625813, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03889418261337884, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.037885240338082556, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03687210015575342, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.03651858632794664, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.035589294319477265, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.03607803271918357, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.03665563544329209, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.034249688015331195, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.03548271722997291, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.0338429821348643, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.034504347610511354, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.0343300246551067, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.034626309229414676, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.034351735929899575, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.03329080357393132, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.03302493896571141, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.03296002264641508, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.03279718148368824, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.03238617105385925, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.03373938136368613, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.03464395937191535, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.03434097160927103, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.03216736464277853, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.033464595958401885, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.03191953727716132, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.031601371337907226, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.03129349193901201, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.0311519928700939, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.031228966232907922, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.03183279118111616, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.03140699266915834, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.029739433758055107, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.031085209897424602, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.030264693182669108, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.028775631744838968, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.030933074535259716, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.029483123812117155, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.030482167472379116, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.029057093837027308, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.02876611296794837, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.030898602443593968, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.02991366094049019, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.029667035193194316, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.03035164902670474, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.029948159179921392, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.029930480083898654, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.02887203249655947, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.029634578247802166, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.02769718151775342, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.029087733076531674, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.028260686116505274, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.028080416371739362, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02758038216093673, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.027675889597474773, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.02804454963041257, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.02986298770278315, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.027614386162803144, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.027656636354100855, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.02805899399556691, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.028059580139344252, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.027939570454668394, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.028860457052912892, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.02797403344530848, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.02792173753716523, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.027729671495624737, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02756665281574183, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.027269724096301236, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.027721700954097737, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.027760661360395105, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.027996734444853625, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.027679691492002223, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.027247750424320184, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.027735349238861964, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.027677456435712077, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.026980288821873786, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.027521307497650763, Learning Rate: 1e-06\n",
      "Fold 4 R2: 0.5794436931610107\n",
      "\n",
      "Fold 5/10\n",
      "Epoch 1/300, Loss: 0.9084851568258261, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8410399473166164, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8213126870650279, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8048059465009955, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7927369897878622, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7850460425207887, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7772886579549765, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7570267432852636, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7604823218116278, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7401578132110306, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7407448857645446, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7311026597324806, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7266836158836945, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7055904333350025, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7075068007541608, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7022284903103793, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6972801247729531, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6840072183669368, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.670089410452903, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6680509749847122, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6639554462855375, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6614652585379684, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6510517069056064, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6403759231295767, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6437202654307401, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6248481401914283, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6164392617684377, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6215295214441758, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.615882849014258, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.6155139656761025, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5983326359640194, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5821770776676226, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5919822005531455, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5691965422298335, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5605979754200464, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5685048359858839, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5428955298435839, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.539165618298929, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5314788414707666, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5268329789366903, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5238333827332605, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.506034702817096, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4968723368041123, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4971596764612801, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.49406216303004497, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.48744583922096446, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.48530645649644394, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.46626334024381033, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.45860087796102594, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.45204621515696564, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.448158743260782, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.43986452606659904, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.43511263252813603, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.42010907960843435, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.4237570770179169, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.4092972572845749, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.4001997264125679, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3963457421411442, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.4059289497665212, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.38303812489479405, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.37282742163803007, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.37675801259052905, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.36076163151596163, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.3634192091000231, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.34686562569835516, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.33731585797629776, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.3442793245179744, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.3420726157064679, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.32377993324889415, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.3113649334711365, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.31763866562632065, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.30322380099869983, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.2966774451204493, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.2945504228148279, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2882665076965018, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.27626426574550095, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.2740485487104971, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2676893331204789, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.261668436889407, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2607966729357273, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.2546639255707777, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.24767056937459148, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.24736711496039282, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.2379089565593985, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2337744075663482, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.22414649306218834, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.22742992054812516, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.2086728309528737, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.20828686409358738, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.20755325710471673, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.20305214011216466, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.19577272594729556, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.19697332438788837, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.19904109392362304, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.19981934922405437, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.2002755950523328, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.18376800436762314, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.17900504690559604, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.17003926851704151, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.17029297408423846, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.16336914717773848, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.15721581461308878, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.15517361028284965, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.1622923555064805, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.160811606653129, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.1421122801832006, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.1487773790766921, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.1506173207978659, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.13611681042592735, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.14195384754787518, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.14428068669159202, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.13103333634289005, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.1289624674599382, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.12706839368690417, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13184349446357052, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.1293146318838566, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.13072821620521666, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.12359047284986399, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.1290119716638251, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11800379174042351, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.11476620862001105, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.11456073000083995, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.11975115547074547, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.11679177863311165, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.11478310333022589, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.10495426005954984, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.10474338938918294, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.1182843794739699, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10618064840194545, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.10672845267042329, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.09705795984290823, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.09486088227433494, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.09730732705019697, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09045609218787544, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.1010963554152205, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.09665909003985079, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.08465139520696446, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.08803449844634986, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.08894004366254504, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08896807892413079, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.09072716180470926, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08632399401144136, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08361656166896035, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08139286246858066, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.07800674834583379, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.07906989621210701, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.07959346143127996, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.08145573235388044, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07609358109250854, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07652806188864043, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.07418682784596577, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.07318384124885631, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07112402638679818, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.06872044970528988, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.0702027820427961, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.06858674356643157, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.07038874740276156, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.06606931409126596, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06493228556046003, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06881354291793666, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.07069577271991138, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.0713393055845665, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.0724343672111819, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.0642146265393571, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06389529531515098, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06078235536247869, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06152529458079157, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.06128403898092765, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.05898468073787568, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.05875271832263922, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.055990227934303166, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.05644408618158932, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.05632537966476211, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.0582028079447867, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05661631073755554, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05354991956036302, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.0551804681367512, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.0530471235419376, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.051613344987736474, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.05092889961751201, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.051770409592722035, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.04953842902485328, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.048757409488287155, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05245725843536703, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.05051236660986007, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.04793700271675104, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04560095232121552, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.04688754472645778, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04717355447856686, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.04793987861728367, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.045550967719924604, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04887428389319891, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.04554149088791654, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.04415849018606204, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.044442731274079675, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04482603780453718, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04413048218039772, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04241205648153643, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.043018078620109375, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.041424301771234864, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04261071388197096, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.04234286251513264, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04077070218192626, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.03856609601385986, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.039249947789726375, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.04152751148124285, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.03834402372565451, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.03739263956682592, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.03861049998881696, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.037633496678516835, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.03631908840299407, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03883117293538172, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.03817979953711546, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03707658426388155, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.035315805975394916, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.0357126761463624, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.037061403210781795, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03603840609894523, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03517006663016126, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03571772613102877, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.034343875543792035, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.035258856826002084, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03324468162712417, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03511994218901743, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.033191331037426296, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.0341123881502242, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03315887055536614, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.031816954948479616, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.032886269514130644, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.031221669662413718, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.03312224022383931, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.031280497914250895, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.03134674076698249, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.02935281190785426, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.029994293222133116, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.029958146282389193, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.029996829531803916, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.030426781766022308, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.0288858887138246, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.029680770409258105, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.0298896103248566, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.028303337955399403, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.0281252267570058, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.028508433388381063, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.029487132766767392, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.028913977237630496, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.028057293329812303, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.028403990112150772, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.02768901806277565, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.02775939789753926, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.028355874427700344, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.02883293314636508, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.026942944755376893, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.027653946435149714, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.027302478687672674, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.02541806442733807, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.02645160748234278, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.02596554747204992, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.026581696647254727, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.02638566465694693, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.026372209710033633, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.027193686135018928, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.026952529821214797, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.02636651878681364, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.02660798112850023, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.02551170921872688, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.026713808644798737, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.02622461910772173, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.024960769387551502, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.026043327006546758, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.02600260374808236, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.02553660645373637, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.025919603702576854, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.02563858843302425, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.024659574656641182, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.024379694928662685, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.025217896115176285, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.024677342216519616, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.024440689566486245, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.025196836813341214, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.024665994923326034, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.02544269974850401, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.02539962282569348, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.025942133102990404, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.023907486278610893, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.024480410603971423, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.023981256233646147, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.024056470068761066, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.024515474580605573, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02457016362230989, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.02388857765995626, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.023881694184073918, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.025134021274839775, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.0247014300732673, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.024112559925717644, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.0249240138579773, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.02432089162212384, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.0238699080493254, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.02423561642630191, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.02423363859187576, Learning Rate: 1e-06\n",
      "Fold 5 R2: 0.6428514122962952\n",
      "\n",
      "Fold 6/10\n",
      "Epoch 1/300, Loss: 0.9136519009553934, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8427841972701157, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8285100754303268, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8057184415527537, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7940118629721147, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7891228463076339, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7712228516989117, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.758027377762372, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.755911007712159, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7521924716007861, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7398145960856087, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7284293061570276, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7300687447378907, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7115718790247471, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7079476389703871, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7042096068587484, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6997773654853241, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6861375159100641, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6882285607766502, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6733952191057084, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6677653970597666, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6530542241621621, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6473936785625506, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6369812013227728, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.630309439912627, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6270811297471011, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6195842208741587, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.610391815251942, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.6110255661644514, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5906185998192316, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5768083379238467, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.586573553236225, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5634289398978028, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5601253698143778, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5575693106349511, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5402679066114788, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5364620666715163, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5186616238913958, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5176431434818461, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5191756464258025, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5027273112460028, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.4955635131159915, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4775316319133662, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4827580236935917, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.4852627180045164, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.47093681029126616, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.46542057470430304, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.46091822544230693, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.44864071396332755, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.4363991139055807, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.43802838687655293, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.4206598106818863, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.4136842130860196, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.40248126017896435, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.39456957276863386, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.39896797303912007, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.38917923124530646, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.38125600731825526, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.37347807650324666, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3716612714755384, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.35716149342965475, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.34815615547608725, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3439757076999809, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.34260636926451815, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.33507185419903524, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.3167767643551283, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.31896854126000707, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.31227556474601165, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3206903368611879, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.29290126584753207, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.30178780250156984, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.28742881006077875, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.28964372714863545, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.28709864352322834, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2739265280056603, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.26449050235597393, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.263577479350416, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2579968366064603, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.2514357163181788, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2509048845194563, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.22789930127844027, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.23247891966300674, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.23320287711258175, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.21682853657233564, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2229216408125962, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.2176472071228148, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.211257330978973, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.2057203805144829, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.20637689040431492, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.2023455488342273, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.19766848939883558, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.19799064844846725, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.18614512134956407, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.18774297267575807, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.1770790705009352, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.17861476766912243, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.17249785693763178, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.1764572341042229, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.16286135766702362, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.16338878398454643, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.16122932203962834, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.1579248865571203, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.15767380878140655, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.15568772778858112, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.1513154961263077, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.15213843878311448, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.15018513430900213, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.14383969757753082, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.13828885772182972, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.14006152711337125, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.13642416677520244, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.13224401211814035, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.1289443968783451, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.12966943051241622, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.12296862796514849, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.12354386465836174, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.11919034395036818, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.12192108444397963, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.113866993992389, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11348270459831515, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.11736379262012771, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.10878053788520113, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.10662219569652895, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.11007503363527829, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.11082944943557811, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.10032798151803922, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.10411109935633744, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.10296916386371927, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.1008926200432868, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.09682360612138917, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.10040767236223704, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.10000725401730477, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.0973988384291341, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09420812573236755, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09730176665360414, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.09122027532209324, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.08674985325977771, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.08763158090318306, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.08456226386412789, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08344804189061816, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.08752779412684561, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08399883280449276, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08103260053675386, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08155616184201421, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.0811375370602819, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.07777077167094508, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.07915743067860603, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.0759264380211317, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07245664529596703, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07573202285396902, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.07073387069792687, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.07035074906447265, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07116566291904147, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.06969536789044549, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.06897879250441925, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.07796850281802914, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.070318940036659, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.06674129534748537, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06855711571966545, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06641206475375575, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06702740334823162, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.060600568080627464, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.061100433238699466, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.060049699548679066, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.061739968253841884, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.060449605642617504, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.059568060019725484, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.05965875329661973, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.058887076670233206, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.05979188979615139, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.05430220817274685, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.05577019210669059, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.0528497306889371, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.0554554142246518, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.055355675612823875, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05215856348034702, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05618301453658297, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.05429180669067781, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05289484055925019, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.050522658785309975, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.051520681503830074, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.05019042409862144, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.05172391415019579, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05029874412885195, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.05009388282329221, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.04601371476921854, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04888987562418738, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.045073140412569046, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04741146710194365, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.04395853779927085, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.04669470361233512, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04550529853735544, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.042528194929414155, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.04244940145577811, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04283182391354555, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04245404639764677, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04261419306733186, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04244773709981502, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04143640455565875, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04339466508053526, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04095115833267381, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.03987820330960087, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04129759417964688, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.03998060489096974, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.03942346445555928, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.038001157720632193, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.03791613678766202, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.03577110402380364, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.037020331861663466, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.036286811002447635, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.03656746605057505, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03743827199162562, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.03527932816857024, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03641049754864807, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.036089580954064296, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.0360573612719397, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.03631061537167694, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03549409851054602, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03332145085346095, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.033515102194645735, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03448246758949908, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.03256601147070716, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03380074337879314, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03361927331248416, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03218111216644697, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03360879730103137, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.034012444010829625, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.031063940541087826, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.03130173162097418, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.030849516438909725, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.03146299814121633, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.03187364403488515, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.030280046106045004, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.03128225933996182, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.029371184308694887, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.029616491020291666, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.030740476908940304, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.030135005358842355, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.02871073383979405, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.028718431492017794, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.028025840444466734, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.028735171649840813, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.028641775391901596, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.029197806044469907, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.028220299065490312, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.027870609601841696, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.027256621755188025, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.026435289484790608, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.026949751461985746, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.027113321956388558, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.02722786965815327, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.026864573453800587, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.027144698779794234, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.027074023774718937, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.02786486440255672, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.02645669521504565, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.0257353881208957, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.025155266440367398, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.025698301288051697, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.02569266047941733, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.026009065511671804, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.02547350529392686, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.025737173784571358, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.025168621617876277, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.025015275453842137, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.025074650686752947, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.0242346973170208, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.024966580816839314, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.024927788473005537, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.02552405737717695, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.02435314091794853, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.02437339980108074, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.024793424325276026, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.024935559935490542, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.025452194617518895, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.02452687261319613, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02423425170864108, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.024603582659288296, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.024213218307004698, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.024334967537205432, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.023710982461424567, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.023909259186703946, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.02440889777403466, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.024232497096910506, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.023537375799179833, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.024151651931431473, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.024743884271364425, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.0238074067319873, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.023457205559633956, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02344448239648644, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.02403624368760782, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.024128558161326603, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.02423263668919666, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.024152631906768942, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.0239119661052393, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.02456802219341073, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.02388452536933407, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.024162834745985042, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.023753057858800587, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.023955929340629636, Learning Rate: 1e-06\n",
      "Fold 6 R2: 0.6392247080802917\n",
      "\n",
      "Fold 7/10\n",
      "Epoch 1/300, Loss: 0.8994957578333118, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8467947508715377, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.824286080613921, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8113031198706808, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7952062366883966, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7892905428439756, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7748399693754655, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7678857815416553, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7514247849017759, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7492794032338299, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7344307597679428, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7296270516854299, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7230409738383715, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7214445562302312, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7139766491666625, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7068840276591385, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6936254305175588, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6852140400228621, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6805115549624721, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6809104179279714, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6620060039471977, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6573840836180916, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6404816749729688, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6350806042363372, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6293091868297963, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6266226964660838, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6146326151829732, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6152059179318102, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.5979206969466391, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5907630384722843, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.572989083920853, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.572866937782191, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5660134927381443, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5614895937563498, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5503825251060196, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5469306024569499, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5229983944681627, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5194325077382824, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5245059343832957, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5019162489643579, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5039617475829546, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.4894020070758047, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4920062664188916, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4762824155107329, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.46645589531222476, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4621270676202412, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.46120683641373356, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.4478271245201932, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.4412159666984896, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.4265042294429827, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.4203157300436044, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.4220282593859902, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.41382108968269976, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.39692984577975693, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.40918760737286336, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.39031500378741496, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.39346264755424065, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3894645194464092, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.3729705844498888, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.35770148944251146, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.36118521833721595, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.3603622462553314, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3399549266205558, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.33206103893020483, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.3279715144558798, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.3221338757231266, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.3110285024099712, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.30720569231087647, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3048297836433483, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.3073596656322479, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.29725664250458345, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.28886137883874435, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.2816347838202609, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.2684561657377436, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2574140157880662, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.25811352929736997, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.2572538128382043, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.25928946518445317, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.24953752454323105, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.24292964120454427, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.23235324364674242, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.22101049257230154, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.2265128142471555, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.22708367037622235, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2205901426982276, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.2107335828904864, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.2091341301610198, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.2131254273124888, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.20006137124345272, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.197698220233374, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.19385897679419456, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.18792254656930513, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.1898059758204448, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.17841507268102863, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.1848571093022069, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.17123600406737266, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.17608458784562123, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.1637228515140618, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.16372041328798367, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.1668253140170363, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.1575857941297036, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.16151500700772564, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.16000701175837578, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.14936162625687033, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.1569909484514707, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.15563323726005193, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.15228155651424505, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.14581343192088453, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.13857323506587668, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.13524082736878457, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.13547730907986436, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.13518293475425697, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.12984717891940586, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.11998291417390486, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.12036590872308876, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.11971581458479544, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.11405400630039504, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.11466364024937907, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.11207365980253944, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11079126973695393, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.10579346554188788, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.10610001996348176, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.10683405342735822, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.10525711965334567, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.1015421737315534, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.10282606640948525, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.09838440712494186, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.1129538765813731, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10448461648406862, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.09708557735326924, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.09587791986480544, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.09005393262339544, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.08962429748683036, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09007006053683124, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.08972900822947297, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.08561156105391587, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.09038819367938404, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.0848236635233028, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.10010177095102359, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.08466142840400527, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.0867605982230434, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08034510688879822, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.0764661534111711, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.07900752702468558, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.07380318702964843, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.08328774778903285, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.07759077163248122, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.07782067806471753, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07418245968373516, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07258740048619765, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.0693526970151859, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.07232053573184376, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.06989826493059532, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.06753355920126167, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.06896823748380324, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.06687480890298192, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.06302282581978207, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.0671788184797462, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06390428991068768, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06186916240597073, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06566784252660184, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.062471272305974476, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06090380105225346, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06342619865001002, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.059811237114894236, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06000171483883375, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.0592905870061132, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.05537357624573044, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.056753935504563244, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.054691984117785586, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.05902504727621622, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.05435242305828046, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.054053266497352453, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.054790101473844506, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05362333378554145, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05053682436671438, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05233020236409163, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.05076070139302483, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05016465631278255, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.04777863923507401, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.0484932169223888, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.045377149423466455, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.049434609569703476, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.051608320797168754, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.04757061397916154, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.048324070564365086, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.04507549804977224, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.043918517429994634, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04667877151241785, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.04587911540948892, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.04414819456825528, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.044034873147176794, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.041916893040643464, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.043969405319871785, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04261109732751605, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04350049817298032, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04085804960584339, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.03949493459791322, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04185051909541782, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04027884425241736, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.039703014813646485, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.03966508435580549, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.03882183042602449, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.03904883697911908, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.03627593070268631, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.03748630180577688, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.03723966653305519, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.036271022114006776, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.03564496010636227, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.036312593901647795, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.036103640836250936, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.035984257094656365, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.03360174299229549, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03440019259630125, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.03407045788591421, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03534881513612934, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.034003721196440205, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03415292272745054, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03476530952449841, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.032906366229245934, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03240904278015789, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.03334810240547868, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03208199978063378, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03152218852427941, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03178984627033336, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03173186153739314, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03169015309290041, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.030410286157002933, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.030497516918031476, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.031171138575182684, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.029986278920234005, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.030627388719327842, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.030378460388960717, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.02851190521747251, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.028716872717383542, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.030421069245549697, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.03096247160264963, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.029671251325856282, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.02737475122926356, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.028812497288365907, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.02773385942925381, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.02870918461416341, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.02723418876434429, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.02839813152728956, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.02707084744602819, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.027164644314141215, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.02660590667230419, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.02637795349465141, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.027570078691727, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.027341451118641263, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.026947775761349293, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.02716858463385437, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.02658704926460227, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.02594823054261977, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.025331010558654235, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.026046674439235577, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.0253419819040389, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.025996788581715353, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.025127376771614522, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.02461327190357673, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.026274396401323095, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.024112029284050193, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.024874945383377468, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.02601891706544387, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.025090343472135217, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.024633720965136455, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.024543006020256237, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.02426007873366905, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.024244924405990523, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.0234726312558485, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.023679317812187763, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.024103896668817425, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.023856610177627094, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.02382333267810224, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.024408570702977573, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.0236659108460704, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02413989459694941, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.023674490331094478, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.023416715591580053, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.024874029916864406, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.023649149597917175, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.02330532978890063, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.023311075966663753, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.024285533819111842, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.023687658371710326, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.022283069860142998, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.02334911837183599, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.02449995805096777, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.02325812512655047, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02426908133386434, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.023560776233767407, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.02314989688464358, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.023325731011131143, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.023274776372539847, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.023229902893116203, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.02400299291350419, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.022912551719648174, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.02279074897966053, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.0248597148994479, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.02313929901140023, Learning Rate: 1e-06\n",
      "Fold 7 R2: 0.5828195214271545\n",
      "\n",
      "Fold 8/10\n",
      "Epoch 1/300, Loss: 0.9048141090175773, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8425513370127617, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8189839969707441, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8089011477518685, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7937212028080904, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7931970297535763, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.791233168372625, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7708492452585245, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7614098410063153, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7545998390716843, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7393896753274942, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7293132370031332, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7214197086382516, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7246146964121468, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7132091046888617, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.7076886963995197, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6896480121944524, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6854241588447667, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.683043014399613, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6692013333115396, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6827011402649216, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6678519679021232, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6579922652697261, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6359919173808037, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6431947229783747, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6301081693625148, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6218468595909167, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6281191438813752, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.6178890617587899, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.600313150807272, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5986872315406799, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5827406350570389, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5817345388327972, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5744286420979078, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5624720605113839, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5636017401761646, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5571692027623141, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5466457141351097, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5297375242166882, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5301635201218762, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5326854296122925, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.5179096524473987, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.5005520559564421, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4992379172693325, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.4865514368196077, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4815459009967273, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.4727941774114778, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.47287478741211225, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.45736870584608635, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.4486184018322184, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.4478916774822187, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.44500204958493195, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.43158590001395986, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.44018203585962706, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.41822733494299874, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.4076012169258504, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.4050692840467525, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3970212853407558, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.39477414043643805, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3862419720691971, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.3816825475873826, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.37341873027101347, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3737719489049308, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.36412533929076374, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.36137838906879666, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.3507747569038898, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.3373260366011269, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.33610419193400615, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.345141882571993, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.3252190806065934, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.3170322545721561, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.3137881057926371, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.30076017787184894, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.29664687340772605, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.29739419267147404, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.2956703359944911, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.28243308335165435, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2780285204135919, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.2809106560447548, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2695742849307724, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.2689774468352523, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.2551393135438991, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.25008752696876285, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.2503677099188672, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.24279449427429634, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.24133425875555112, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.22860212642935257, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.2291924853490878, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.2254273172798036, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.2112541806094254, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.21613581478595734, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.21016832092140295, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.20804244430759286, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.19888223349293577, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.19307994993427133, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.20128738370877278, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.2003987135389183, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.18455194144309323, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.18561106873086736, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.1746481811698479, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.17942917846803424, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.17793504082703893, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.17679954612556892, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.17864318229729617, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.167008121179629, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.16117265443258647, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.16163698442374605, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.16340759670055366, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.14878706058746652, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.15220774068862578, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.14562638849020004, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.15463883382610127, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.14614701563421684, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.13870492398361617, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13723126506503625, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.1302833520347559, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.1296350269377986, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.1297479890569856, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.1339872651085069, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.12617963703372811, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.1263717115868496, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.1261305846745455, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.12313868548673919, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.1243168841811675, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.11661365533931346, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.11254890291373941, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.11333121085845971, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.11392989698090131, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.11275676788785789, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.11500318131492107, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.10976990312337875, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.10959030716102335, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.10504263601725615, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.1038364112565789, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.10187012758813327, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.10361298102932641, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.09782524317314353, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.10118341709994062, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.09531858016418505, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.09370990521922896, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.08797871359164201, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08712535299643685, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.086720710408084, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08609306562361838, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.09045477631159976, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.08728898024257226, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.08587922962217391, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.08647713890369935, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07934723525673529, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.08038742331009877, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.08679646253585815, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.0779555935554112, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07764966474680961, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.07818240078189705, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.08068802410477324, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.07760520359571976, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.07570857281171822, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.07258626446127892, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.07810268857622449, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.0715405179447011, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06877649669783024, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.06799805767928498, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.07253755016040199, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.0683096257757537, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.0654868285018432, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06791985817725145, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06835358929407748, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.06502056088817271, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.06375613304067261, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.06218261644244194, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.06264202447631691, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.06109019618811486, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.0609058487358727, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.059528592832480805, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05855352143886723, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05925646410146846, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05778294231129598, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.0588043494315087, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05855424023127254, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.056310520707806455, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.057270449833779394, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.05453530675436877, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.055071178210687036, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05222143699662595, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.051376001178463804, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.05167847654864758, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.05272563683647144, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.050139286850072164, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.049869037669482114, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.05089173185391516, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.04941863514765908, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.047800352018845235, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.04993532437689697, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.04754272912027715, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.047659844537324546, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.045552893671431116, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04555221675317499, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04870168704398071, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04778108543044404, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04719863671667968, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.043497609493287306, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.04330640979394128, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04494102494909039, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04391700814513466, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.04224346469663366, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.041266039980551866, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.0398151696058391, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.0403015775299525, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.0397451925032501, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.04259523438124717, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.041340408469491365, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.040808293048905424, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.037778639552902574, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.040947635622718664, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.03984962325967566, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.0387012934241491, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.038410952574090114, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.037827282082996794, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03785608888049669, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03539658118558081, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03635140025068687, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.037503591726852366, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03606615625793421, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03691611787940882, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03503005098127111, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03650362478404105, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03383979128225695, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.03510096961561637, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.03340889122101325, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.034468603662297696, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.033666050179472454, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.03296943749242191, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.03448296293521984, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.0322807741976237, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.034054799052544785, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.03283061426651629, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.031896675807199903, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.031274995660480066, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.03177090427732166, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.031400608724053904, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.032175679349257975, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.031164201117957695, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.03090422751405571, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.030892736997596825, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.03044489121559677, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.031718340997077245, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.03154579823530173, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.030508544673270816, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.030313420569217656, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.030825423901971384, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.027543570940630344, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.029648303089639807, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.029673265316818333, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.029099676805206493, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.028312193371261223, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.027778054528598543, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.02933615969517563, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.0276158896239498, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.028870530072835427, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.028577945584171933, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.02780567003484768, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.028465966328601294, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.02942691757520543, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.027935085794593716, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.027381587632094757, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.029184549127387095, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.02662015560118458, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.027408757258819628, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.028020812363564213, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.027963126573381545, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.02713215032804616, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.027894296718737745, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.02756084791750093, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.027026247751863696, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.02613391602246822, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.02669097421855866, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.026467202608532543, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.02633430947891519, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.026597698018709315, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.026817281624373003, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.026944297342266464, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.028307405972405324, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.02683980081560491, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.02605728328793864, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.026140728352379194, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.02691249945495702, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.026261179024188577, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.026087243440030498, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.026957732589938974, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.02628875687671236, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.026202339902991735, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.025601581729288343, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.02571903088990646, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.02616470486302919, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.027183476776544806, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.02613381112489519, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.025751562836238102, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.026427055178563807, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.027235824496874325, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.026163724027102507, Learning Rate: 1e-06\n",
      "Fold 8 R2: 0.6541426181793213\n",
      "\n",
      "Fold 9/10\n",
      "Epoch 1/300, Loss: 0.9068048226682446, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8327631165709677, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8234317287614074, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8106142139133019, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.804336515408528, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7822091368180287, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7768772297267672, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7633021526698824, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7578961456878276, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7460162654707704, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7301136990891227, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7365789104111587, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7267237969591648, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7093236235123647, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7085542912724652, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.6900928620296188, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6784430168852021, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6738103164147727, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6693173639381989, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6638902916183954, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.652788676038573, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6485157658027697, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6345863538452342, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6224380954156948, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6288842098622383, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6164553950104532, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6184620540353316, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.603390584640865, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.5869103854970087, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.585911642147016, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5711409042153177, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5719037089921251, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.5618036370488662, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5565467186366455, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.540109824530686, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5333820958680744, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5310191112228587, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5198961224737046, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5070617647865151, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5066088468213624, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5106166174894646, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.4969619112678721, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.4831613920911958, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.47879139230221135, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.46165210612212554, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4442842961112155, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.4428517984438546, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.4370716942262046, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.43647147130362596, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.428754143699815, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.43462201122996175, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.41802922136421444, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.40406487486030485, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.4090652277198019, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.3889543798905385, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.3906593799968309, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.386272399674488, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.377592394434953, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.36733872641490983, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3641741147524194, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.36335588689846327, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.3490461535468886, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3414390781257726, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.34558501594428775, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.33320301375057126, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.32315219354025926, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.31901953454259074, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.32573238090623785, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.29789187523383126, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.29685615965082673, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.29037388668784614, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.3021678090850009, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.29328177793870996, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.27808808393870726, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.27978383325323275, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.2726004506591, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.2602849551771261, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2629958717506143, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.2518949269116679, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.24992427380779123, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.24434851694710646, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.24869328020494194, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.23554344003713584, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.24135050177574158, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2240288104814819, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.23180240977414046, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.2195255573414549, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.2138644526653652, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.21081351668019838, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.2042787184443655, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.20836126295071614, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.19483959816301924, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.194838306005997, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.18757533555543876, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.18490952526844, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.17483104557930668, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.19025686439834064, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.17778280707477015, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.1789006937908221, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.18429074860826322, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.164427718972858, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.170453027739555, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.15687493272597278, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.15670658421667316, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.16260846543915664, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.1503688765666153, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.14501219836971427, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.14779922818835778, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.14276184830107266, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.1466471811261358, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.14386905522286136, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.1386877019194108, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.1402875004878527, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.12279077364674097, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13122024981281424, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.1334623793446565, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.12903730118576484, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.1259558691254145, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.11878662122578561, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.11795987518905085, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.12102265589976613, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.12334530923185469, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.11736448495825634, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.11007633880723881, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.10432398055173174, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.10745055728320833, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.10767522143034995, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.1043360683171055, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10125031086462963, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.09575212294165092, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.09986724487588375, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.09621814157389387, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.10144324737447727, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09713932230502745, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09442029502007025, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.09583014306388324, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.09398990195197394, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.08853397617423081, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.08412366082208066, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.0897640736797188, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.0867141181060785, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08384704910501649, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08612536233437212, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08281004806108112, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.08012641348604914, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.0788766857755335, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.07846530262805239, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.07884016878242735, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.07533138627304306, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.07564955407494231, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.07699147838203213, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.0692456687175775, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07344402754797211, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.07587727201701719, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.07349062909053851, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.06939707154148742, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.07059213344620753, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.07078223934845079, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06656589146844949, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.06837029519337642, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.06836204447701008, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.06730923094326936, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06455697397453876, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06254092566197432, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.05799279998563513, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06262022704829144, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06040973975499974, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.0604412492034556, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.05902034520537038, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.06189299799218963, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.06082727778938752, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.056842526538839824, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.05689246613013593, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.05702629812721965, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.05803717073949077, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05368199086264719, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.053292274192164216, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.054908205979043924, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.05113705987983112, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.05356554081074045, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.05223822508808933, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.05218560937084729, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.05123001777956003, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05196891743925553, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.04723020463804655, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.04849691846985606, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.049480046605384805, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.04660791599580759, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.047621689901887615, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.048832440277255036, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.04635364647153058, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04606959405296211, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.04479977868120127, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.04494094454883775, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.04611510255291492, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.04445145033960101, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04182391509979586, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04312982509219194, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.043976426171728325, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04313690330880352, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04392431445325477, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.041485918355704865, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04062923930491073, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04145365382862996, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.0392199509741762, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.038935331769193275, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.037938687903217125, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.03911186886739127, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.04099550803156593, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.0379903900991135, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.03877219477597671, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03617548206938973, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.03920297864588756, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.03870913827249521, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.036550935027720055, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03757713386152364, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.03601608413589906, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03714693625327907, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03576945362589027, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03452715890694268, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.0348026140628359, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.03485567252375657, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.03360778109842463, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.03356796972264972, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03345433035511759, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.034535692910416214, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03360373017531407, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.030826914371757566, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.0327488522080681, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.03194289718153356, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.03303059431005128, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.03205624762686748, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.03116445271651956, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.031189187653834306, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.02999544200263446, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.030767249061337, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.0306756330441825, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.02853060343974753, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.0297266430445487, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.030111363263730005, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.029861285787405847, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.030289394686682316, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.02896720159185838, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.028485279978264735, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.030658293633332737, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.02933917477538314, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.029145604895451403, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.02956987948074371, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.028903655519213856, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.027454048135801205, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.028726947458484507, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.028024627650274508, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.026953699326590648, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.02665598999378802, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.028145799012501027, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.02687412212732472, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.027659428765690778, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.02658293870148025, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.026431367225662063, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.02828521196600757, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.02628351493349558, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.027008232342291483, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.026539902839385256, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.026867194961803623, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.026969910115946696, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.026395679816980905, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.025956295430660248, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.026098256718508805, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.026369110409972033, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.02654416637518738, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.025372575379059285, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.025213655838860743, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.02631549938004228, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.026368514622879934, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.02579242368287678, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.025037175622073155, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.025351658369166943, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.02520598909711536, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.025752889723340167, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.026547673264447645, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.026092767456098447, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.024738647585994082, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.023757925021308888, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.024412428264659416, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.024675770874925052, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.024943342455957508, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.02448162392866385, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.024344851015300692, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.024796201163737833, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.025424289128071147, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.026221228374427634, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.02487156333849777, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.024793900478678414, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.02494437782730483, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.025550023440413083, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.026090117973051493, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.024692494988064223, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.02524108107237122, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.024349406573779975, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.02487917833879024, Learning Rate: 1e-06\n",
      "Fold 9 R2: 0.6065989136695862\n",
      "\n",
      "Fold 10/10\n",
      "Epoch 1/300, Loss: 0.8790647802473623, Learning Rate: 0.0009999726120980734\n",
      "Epoch 2/300, Loss: 0.8389974945708166, Learning Rate: 0.0009998904513956854\n",
      "Epoch 3/300, Loss: 0.8219955993604057, Learning Rate: 0.0009997535269026829\n",
      "Epoch 4/300, Loss: 0.8092217098308515, Learning Rate: 0.0009995618536343797\n",
      "Epoch 5/300, Loss: 0.7904567130004303, Learning Rate: 0.0009993154526099096\n",
      "Epoch 6/300, Loss: 0.7745696804191493, Learning Rate: 0.0009990143508499217\n",
      "Epoch 7/300, Loss: 0.7672819291489034, Learning Rate: 0.0009986585813736167\n",
      "Epoch 8/300, Loss: 0.7628240298621262, Learning Rate: 0.0009982481831951274\n",
      "Epoch 9/300, Loss: 0.7489328535297249, Learning Rate: 0.0009977832013192385\n",
      "Epoch 10/300, Loss: 0.7420744790306574, Learning Rate: 0.0009972636867364526\n",
      "Epoch 11/300, Loss: 0.7404114958606188, Learning Rate: 0.0009966896964173982\n",
      "Epoch 12/300, Loss: 0.7218092808240577, Learning Rate: 0.000996061293306582\n",
      "Epoch 13/300, Loss: 0.7205151772197289, Learning Rate: 0.0009953785463154864\n",
      "Epoch 14/300, Loss: 0.7114761879172506, Learning Rate: 0.0009946415303150131\n",
      "Epoch 15/300, Loss: 0.7100806402254708, Learning Rate: 0.0009938503261272718\n",
      "Epoch 16/300, Loss: 0.6977206589300421, Learning Rate: 0.0009930050205167178\n",
      "Epoch 17/300, Loss: 0.6950887147384354, Learning Rate: 0.0009921057061806368\n",
      "Epoch 18/300, Loss: 0.6873266938366468, Learning Rate: 0.0009911524817389807\n",
      "Epoch 19/300, Loss: 0.6723323660560802, Learning Rate: 0.0009901454517235507\n",
      "Epoch 20/300, Loss: 0.6734398135656043, Learning Rate: 0.0009890847265665364\n",
      "Epoch 21/300, Loss: 0.6607726431345637, Learning Rate: 0.0009879704225884047\n",
      "Epoch 22/300, Loss: 0.6508947784387613, Learning Rate: 0.000986802661985144\n",
      "Epoch 23/300, Loss: 0.6478620707234249, Learning Rate: 0.0009855815728148636\n",
      "Epoch 24/300, Loss: 0.6430388096767136, Learning Rate: 0.0009843072889837517\n",
      "Epoch 25/300, Loss: 0.6361937379535241, Learning Rate: 0.00098297995023139\n",
      "Epoch 26/300, Loss: 0.6145035707497899, Learning Rate: 0.0009815997021154308\n",
      "Epoch 27/300, Loss: 0.6060386492481714, Learning Rate: 0.0009801666959956335\n",
      "Epoch 28/300, Loss: 0.6065552958959266, Learning Rate: 0.000978681089017268\n",
      "Epoch 29/300, Loss: 0.597620241249664, Learning Rate: 0.0009771430440938809\n",
      "Epoch 30/300, Loss: 0.5902416668360746, Learning Rate: 0.0009755527298894299\n",
      "Epoch 31/300, Loss: 0.5815545290331298, Learning Rate: 0.0009739103207997887\n",
      "Epoch 32/300, Loss: 0.5738826571386072, Learning Rate: 0.0009722159969336225\n",
      "Epoch 33/300, Loss: 0.575380211766762, Learning Rate: 0.0009704699440926363\n",
      "Epoch 34/300, Loss: 0.5573665062083474, Learning Rate: 0.0009686723537512004\n",
      "Epoch 35/300, Loss: 0.5519324778756008, Learning Rate: 0.0009668234230353527\n",
      "Epoch 36/300, Loss: 0.5566873678678199, Learning Rate: 0.0009649233547011821\n",
      "Epoch 37/300, Loss: 0.5388132869442807, Learning Rate: 0.000962972357112593\n",
      "Epoch 38/300, Loss: 0.5337090375302713, Learning Rate: 0.0009609706442184566\n",
      "Epoch 39/300, Loss: 0.5373692305027684, Learning Rate: 0.0009589184355291491\n",
      "Epoch 40/300, Loss: 0.5132159707666952, Learning Rate: 0.0009568159560924797\n",
      "Epoch 41/300, Loss: 0.5121595116355752, Learning Rate: 0.0009546634364690113\n",
      "Epoch 42/300, Loss: 0.5059486393687092, Learning Rate: 0.0009524611127067773\n",
      "Epoch 43/300, Loss: 0.49655543097966837, Learning Rate: 0.0009502092263153963\n",
      "Epoch 44/300, Loss: 0.4831629489796071, Learning Rate: 0.0009479080242395872\n",
      "Epoch 45/300, Loss: 0.48476879053478, Learning Rate: 0.0009455577588320901\n",
      "Epoch 46/300, Loss: 0.4727747776085817, Learning Rate: 0.0009431586878259921\n",
      "Epoch 47/300, Loss: 0.46721871517881564, Learning Rate: 0.0009407110743064639\n",
      "Epoch 48/300, Loss: 0.45562979201727277, Learning Rate: 0.0009382151866819102\n",
      "Epoch 49/300, Loss: 0.4491595969169955, Learning Rate: 0.0009356712986545351\n",
      "Epoch 50/300, Loss: 0.44071549328067633, Learning Rate: 0.0009330796891903276\n",
      "Epoch 51/300, Loss: 0.4322273010694528, Learning Rate: 0.0009304406424884703\n",
      "Epoch 52/300, Loss: 0.436148464679718, Learning Rate: 0.0009277544479501735\n",
      "Epoch 53/300, Loss: 0.4310370121575609, Learning Rate: 0.000925021400146939\n",
      "Epoch 54/300, Loss: 0.42480369949642616, Learning Rate: 0.000922241798788257\n",
      "Epoch 55/300, Loss: 0.40745932882345176, Learning Rate: 0.0009194159486887398\n",
      "Epoch 56/300, Loss: 0.39922706348986564, Learning Rate: 0.0009165441597346951\n",
      "Epoch 57/300, Loss: 0.3866436183452606, Learning Rate: 0.000913626746850144\n",
      "Epoch 58/300, Loss: 0.3917372968000702, Learning Rate: 0.0009106640299622855\n",
      "Epoch 59/300, Loss: 0.3831821695158753, Learning Rate: 0.0009076563339664131\n",
      "Epoch 60/300, Loss: 0.3730519981701163, Learning Rate: 0.0009046039886902866\n",
      "Epoch 61/300, Loss: 0.36495415922961655, Learning Rate: 0.000901507328857962\n",
      "Epoch 62/300, Loss: 0.3511447368920604, Learning Rate: 0.0008983666940530862\n",
      "Epoch 63/300, Loss: 0.3454814709817307, Learning Rate: 0.0008951824286816575\n",
      "Epoch 64/300, Loss: 0.34590490596203866, Learning Rate: 0.0008919548819342571\n",
      "Epoch 65/300, Loss: 0.3377051483603972, Learning Rate: 0.000888684407747757\n",
      "Epoch 66/300, Loss: 0.33501523167272157, Learning Rate: 0.0008853713647665067\n",
      "Epoch 67/300, Loss: 0.32948497956312156, Learning Rate: 0.0008820161163030039\n",
      "Epoch 68/300, Loss: 0.3250818891993052, Learning Rate: 0.0008786190302980523\n",
      "Epoch 69/300, Loss: 0.3123630809633038, Learning Rate: 0.0008751804792804145\n",
      "Epoch 70/300, Loss: 0.2979750312581847, Learning Rate: 0.0008717008403259584\n",
      "Epoch 71/300, Loss: 0.2994522757922547, Learning Rate: 0.0008681804950163072\n",
      "Epoch 72/300, Loss: 0.29760576728024063, Learning Rate: 0.000864619829396995\n",
      "Epoch 73/300, Loss: 0.28526039364971695, Learning Rate: 0.0008610192339351319\n",
      "Epoch 74/300, Loss: 0.29186316739909257, Learning Rate: 0.0008573791034765853\n",
      "Epoch 75/300, Loss: 0.2852468105811107, Learning Rate: 0.0008536998372026805\n",
      "Epoch 76/300, Loss: 0.26389313630665406, Learning Rate: 0.0008499818385864262\n",
      "Epoch 77/300, Loss: 0.26373562235620956, Learning Rate: 0.0008462255153482682\n",
      "Epoch 78/300, Loss: 0.2602741725837128, Learning Rate: 0.00084243127941138\n",
      "Epoch 79/300, Loss: 0.25368085013160224, Learning Rate: 0.000838599546856489\n",
      "Epoch 80/300, Loss: 0.2605268236579774, Learning Rate: 0.0008347307378762497\n",
      "Epoch 81/300, Loss: 0.25306178063531465, Learning Rate: 0.0008308252767291641\n",
      "Epoch 82/300, Loss: 0.24632365699810319, Learning Rate: 0.0008268835916930578\n",
      "Epoch 83/300, Loss: 0.24090530034861987, Learning Rate: 0.0008229061150181133\n",
      "Epoch 84/300, Loss: 0.2403925032932547, Learning Rate: 0.0008188932828794705\n",
      "Epoch 85/300, Loss: 0.2362421960393085, Learning Rate: 0.0008148455353293938\n",
      "Epoch 86/300, Loss: 0.22860895493362524, Learning Rate: 0.0008107633162490161\n",
      "Epoch 87/300, Loss: 0.22857636967791786, Learning Rate: 0.0008066470732996618\n",
      "Epoch 88/300, Loss: 0.21830198489412478, Learning Rate: 0.0008024972578737563\n",
      "Epoch 89/300, Loss: 0.20333921607536606, Learning Rate: 0.000798314325045325\n",
      "Epoch 90/300, Loss: 0.19974572956562042, Learning Rate: 0.0007940987335200902\n",
      "Epoch 91/300, Loss: 0.21028256397458572, Learning Rate: 0.0007898509455851679\n",
      "Epoch 92/300, Loss: 0.19768911119126067, Learning Rate: 0.0007855714270583736\n",
      "Epoch 93/300, Loss: 0.19453836600237254, Learning Rate: 0.0007812606472371392\n",
      "Epoch 94/300, Loss: 0.19173027263789238, Learning Rate: 0.0007769190788470502\n",
      "Epoch 95/300, Loss: 0.19036522800032096, Learning Rate: 0.0007725471979900058\n",
      "Epoch 96/300, Loss: 0.18637888708823844, Learning Rate: 0.0007681454840920086\n",
      "Epoch 97/300, Loss: 0.18230579472795316, Learning Rate: 0.0007637144198505904\n",
      "Epoch 98/300, Loss: 0.18262349427500857, Learning Rate: 0.0007592544911818785\n",
      "Epoch 99/300, Loss: 0.18109766129828705, Learning Rate: 0.0007547661871673104\n",
      "Epoch 100/300, Loss: 0.1695904429011707, Learning Rate: 0.0007502499999999999\n",
      "Epoch 101/300, Loss: 0.1713859634120253, Learning Rate: 0.0007457064249307628\n",
      "Epoch 102/300, Loss: 0.16732531965156144, Learning Rate: 0.0007411359602138068\n",
      "Epoch 103/300, Loss: 0.16840259598780283, Learning Rate: 0.0007365391070520926\n",
      "Epoch 104/300, Loss: 0.16005668176125876, Learning Rate: 0.0007319163695423711\n",
      "Epoch 105/300, Loss: 0.1608666973777964, Learning Rate: 0.0007272682546199037\n",
      "Epoch 106/300, Loss: 0.15995653209429753, Learning Rate: 0.0007225952720028713\n",
      "Epoch 107/300, Loss: 0.16572153436232218, Learning Rate: 0.0007178979341364777\n",
      "Epoch 108/300, Loss: 0.15365315870016436, Learning Rate: 0.0007131767561367538\n",
      "Epoch 109/300, Loss: 0.1493229483124576, Learning Rate: 0.0007084322557340705\n",
      "Epoch 110/300, Loss: 0.14680490803114976, Learning Rate: 0.0007036649532163622\n",
      "Epoch 111/300, Loss: 0.14406859733258623, Learning Rate: 0.0006988753713720729\n",
      "Epoch 112/300, Loss: 0.1362822301025632, Learning Rate: 0.0006940640354328255\n",
      "Epoch 113/300, Loss: 0.13819730008327508, Learning Rate: 0.0006892314730158244\n",
      "Epoch 114/300, Loss: 0.1404207533673395, Learning Rate: 0.0006843782140659968\n",
      "Epoch 115/300, Loss: 0.13581841080626356, Learning Rate: 0.0006795047907978774\n",
      "Epoch 116/300, Loss: 0.14009714673591567, Learning Rate: 0.0006746117376372467\n",
      "Epoch 117/300, Loss: 0.13824532454526878, Learning Rate: 0.0006696995911625232\n",
      "Epoch 118/300, Loss: 0.1315999691244922, Learning Rate: 0.0006647688900459223\n",
      "Epoch 119/300, Loss: 0.1266942506160917, Learning Rate: 0.0006598201749943859\n",
      "Epoch 120/300, Loss: 0.12462471218048772, Learning Rate: 0.0006548539886902863\n",
      "Epoch 121/300, Loss: 0.12198900205047825, Learning Rate: 0.0006498708757319154\n",
      "Epoch 122/300, Loss: 0.11722945761454256, Learning Rate: 0.0006448713825737637\n",
      "Epoch 123/300, Loss: 0.11683543441416341, Learning Rate: 0.0006398560574665952\n",
      "Epoch 124/300, Loss: 0.12019088788877559, Learning Rate: 0.0006348254503973253\n",
      "Epoch 125/300, Loss: 0.116566276059875, Learning Rate: 0.0006297801130287094\n",
      "Epoch 126/300, Loss: 0.11754481173768828, Learning Rate: 0.000624720598638845\n",
      "Epoch 127/300, Loss: 0.10583434093602095, Learning Rate: 0.0006196474620605013\n",
      "Epoch 128/300, Loss: 0.10904538089150115, Learning Rate: 0.0006145612596202727\n",
      "Epoch 129/300, Loss: 0.10666264244649984, Learning Rate: 0.0006094625490775732\n",
      "Epoch 130/300, Loss: 0.10261829386029063, Learning Rate: 0.0006043518895634709\n",
      "Epoch 131/300, Loss: 0.10020626138282727, Learning Rate: 0.0005992298415193734\n",
      "Epoch 132/300, Loss: 0.103184381334842, Learning Rate: 0.0005940969666355696\n",
      "Epoch 133/300, Loss: 0.10568292349388328, Learning Rate: 0.0005889538277896319\n",
      "Epoch 134/300, Loss: 0.09801415806706948, Learning Rate: 0.0005838009889846931\n",
      "Epoch 135/300, Loss: 0.09634348542629918, Learning Rate: 0.0005786390152875953\n",
      "Epoch 136/300, Loss: 0.10136107789187492, Learning Rate: 0.0005734684727669246\n",
      "Epoch 137/300, Loss: 0.10020490636742568, Learning Rate: 0.000568289928430935\n",
      "Epoch 138/300, Loss: 0.0982350700452358, Learning Rate: 0.0005631039501653699\n",
      "Epoch 139/300, Loss: 0.09245377849740317, Learning Rate: 0.0005579111066711869\n",
      "Epoch 140/300, Loss: 0.09136084618070457, Learning Rate: 0.000552711967402193\n",
      "Epoch 141/300, Loss: 0.09129925109917604, Learning Rate: 0.0005475071025025979\n",
      "Epoch 142/300, Loss: 0.08904236041103737, Learning Rate: 0.0005422970827444916\n",
      "Epoch 143/300, Loss: 0.08551372894192044, Learning Rate: 0.000537082479465252\n",
      "Epoch 144/300, Loss: 0.08752840879974486, Learning Rate: 0.0005318638645048921\n",
      "Epoch 145/300, Loss: 0.0856797654606119, Learning Rate: 0.0005266418101433505\n",
      "Epoch 146/300, Loss: 0.08606265843669071, Learning Rate: 0.0005214168890377353\n",
      "Epoch 147/300, Loss: 0.08752825163021873, Learning Rate: 0.000516189674159525\n",
      "Epoch 148/300, Loss: 0.0805214410529861, Learning Rate: 0.0005109607387317368\n",
      "Epoch 149/300, Loss: 0.0798615772701517, Learning Rate: 0.0005057306561660648\n",
      "Epoch 150/300, Loss: 0.08129058075667936, Learning Rate: 0.0005005\n",
      "Epoch 151/300, Loss: 0.08031849789468548, Learning Rate: 0.0004952693438339353\n",
      "Epoch 152/300, Loss: 0.07784719298346134, Learning Rate: 0.0004900392612682634\n",
      "Epoch 153/300, Loss: 0.07395645386621921, Learning Rate: 0.00048481032584047495\n",
      "Epoch 154/300, Loss: 0.08110793802557112, Learning Rate: 0.0004795831109622648\n",
      "Epoch 155/300, Loss: 0.07317765269287024, Learning Rate: 0.0004743581898566495\n",
      "Epoch 156/300, Loss: 0.07168597534676141, Learning Rate: 0.00046913613549510807\n",
      "Epoch 157/300, Loss: 0.07284298501437224, Learning Rate: 0.00046391752053474806\n",
      "Epoch 158/300, Loss: 0.07307240833775906, Learning Rate: 0.0004587029172555084\n",
      "Epoch 159/300, Loss: 0.06853967334462117, Learning Rate: 0.0004534928974974021\n",
      "Epoch 160/300, Loss: 0.07209348343784296, Learning Rate: 0.0004482880325978072\n",
      "Epoch 161/300, Loss: 0.07154374673396727, Learning Rate: 0.0004430888933288132\n",
      "Epoch 162/300, Loss: 0.07250669457112686, Learning Rate: 0.0004378960498346301\n",
      "Epoch 163/300, Loss: 0.06605145089988466, Learning Rate: 0.000432710071569065\n",
      "Epoch 164/300, Loss: 0.06649097154223466, Learning Rate: 0.0004275315272330756\n",
      "Epoch 165/300, Loss: 0.06511088762479493, Learning Rate: 0.0004223609847124048\n",
      "Epoch 166/300, Loss: 0.06622511520981789, Learning Rate: 0.0004171990110153068\n",
      "Epoch 167/300, Loss: 0.06326519297082213, Learning Rate: 0.00041204617221036815\n",
      "Epoch 168/300, Loss: 0.06248963204554365, Learning Rate: 0.00040690303336443054\n",
      "Epoch 169/300, Loss: 0.0680973260085794, Learning Rate: 0.00040177015848062643\n",
      "Epoch 170/300, Loss: 0.0659693874701669, Learning Rate: 0.00039664811043652916\n",
      "Epoch 171/300, Loss: 0.06710218180772624, Learning Rate: 0.000391537450922427\n",
      "Epoch 172/300, Loss: 0.06140290431772606, Learning Rate: 0.00038643874037972754\n",
      "Epoch 173/300, Loss: 0.06034840643405914, Learning Rate: 0.00038135253793949894\n",
      "Epoch 174/300, Loss: 0.05860543029406403, Learning Rate: 0.0003762794013611551\n",
      "Epoch 175/300, Loss: 0.059940166252700586, Learning Rate: 0.00037121988697129095\n",
      "Epoch 176/300, Loss: 0.05710347708833369, Learning Rate: 0.00036617454960267493\n",
      "Epoch 177/300, Loss: 0.05735572710444656, Learning Rate: 0.0003611439425334049\n",
      "Epoch 178/300, Loss: 0.0594691945404946, Learning Rate: 0.0003561286174262363\n",
      "Epoch 179/300, Loss: 0.061169216102814374, Learning Rate: 0.0003511291242680847\n",
      "Epoch 180/300, Loss: 0.060807879259691965, Learning Rate: 0.0003461460113097137\n",
      "Epoch 181/300, Loss: 0.05219076382868652, Learning Rate: 0.00034117982500561395\n",
      "Epoch 182/300, Loss: 0.051926774927709675, Learning Rate: 0.00033623110995407757\n",
      "Epoch 183/300, Loss: 0.051751794529300704, Learning Rate: 0.00033130040883747687\n",
      "Epoch 184/300, Loss: 0.05099755270948893, Learning Rate: 0.0003263882623627533\n",
      "Epoch 185/300, Loss: 0.05091923894950106, Learning Rate: 0.00032149520920212257\n",
      "Epoch 186/300, Loss: 0.05078457786312586, Learning Rate: 0.00031662178593400343\n",
      "Epoch 187/300, Loss: 0.051296196901534176, Learning Rate: 0.00031176852698417556\n",
      "Epoch 188/300, Loss: 0.05139704194815853, Learning Rate: 0.00030693596456717455\n",
      "Epoch 189/300, Loss: 0.04782066454144218, Learning Rate: 0.00030212462862792697\n",
      "Epoch 190/300, Loss: 0.04875240515021584, Learning Rate: 0.00029733504678363775\n",
      "Epoch 191/300, Loss: 0.050551873172008537, Learning Rate: 0.0002925677442659297\n",
      "Epoch 192/300, Loss: 0.04846598283399509, Learning Rate: 0.0002878232438632462\n",
      "Epoch 193/300, Loss: 0.049950009235475636, Learning Rate: 0.00028310206586352245\n",
      "Epoch 194/300, Loss: 0.046678013840242276, Learning Rate: 0.00027840472799712883\n",
      "Epoch 195/300, Loss: 0.047111839857659765, Learning Rate: 0.0002737317453800964\n",
      "Epoch 196/300, Loss: 0.045949566708524014, Learning Rate: 0.0002690836304576292\n",
      "Epoch 197/300, Loss: 0.04539318342657783, Learning Rate: 0.00026446089294790756\n",
      "Epoch 198/300, Loss: 0.04449876331830326, Learning Rate: 0.0002598640397861931\n",
      "Epoch 199/300, Loss: 0.04327039132955708, Learning Rate: 0.00025529357506923715\n",
      "Epoch 200/300, Loss: 0.04579152058385595, Learning Rate: 0.0002507499999999999\n",
      "Epoch 201/300, Loss: 0.04548977112656907, Learning Rate: 0.0002462338128326895\n",
      "Epoch 202/300, Loss: 0.04468105674450156, Learning Rate: 0.00024174550881812148\n",
      "Epoch 203/300, Loss: 0.04545529931783676, Learning Rate: 0.0002372855801494095\n",
      "Epoch 204/300, Loss: 0.04226999208802664, Learning Rate: 0.00023285451590799124\n",
      "Epoch 205/300, Loss: 0.04199726360885403, Learning Rate: 0.0002284528020099941\n",
      "Epoch 206/300, Loss: 0.04347319735945025, Learning Rate: 0.0002240809211529496\n",
      "Epoch 207/300, Loss: 0.04063938593468334, Learning Rate: 0.00021973935276286082\n",
      "Epoch 208/300, Loss: 0.04069169917249981, Learning Rate: 0.00021542857294162636\n",
      "Epoch 209/300, Loss: 0.04017480792878549, Learning Rate: 0.00021114905441483182\n",
      "Epoch 210/300, Loss: 0.0417566846443128, Learning Rate: 0.00020690126647990968\n",
      "Epoch 211/300, Loss: 0.03988535398075098, Learning Rate: 0.00020268567495467478\n",
      "Epoch 212/300, Loss: 0.03749696038003209, Learning Rate: 0.0001985027421262437\n",
      "Epoch 213/300, Loss: 0.039641538021873826, Learning Rate: 0.0001943529267003383\n",
      "Epoch 214/300, Loss: 0.038091178794827645, Learning Rate: 0.00019023668375098396\n",
      "Epoch 215/300, Loss: 0.03958876306026042, Learning Rate: 0.00018615446467060623\n",
      "Epoch 216/300, Loss: 0.03868173654603807, Learning Rate: 0.0001821067171205294\n",
      "Epoch 217/300, Loss: 0.03824152439078198, Learning Rate: 0.0001780938849818867\n",
      "Epoch 218/300, Loss: 0.03810630217666113, Learning Rate: 0.00017411640830694239\n",
      "Epoch 219/300, Loss: 0.03730490865020812, Learning Rate: 0.0001701747232708357\n",
      "Epoch 220/300, Loss: 0.03676911544856391, Learning Rate: 0.00016626926212375023\n",
      "Epoch 221/300, Loss: 0.03797194382906714, Learning Rate: 0.00016240045314351093\n",
      "Epoch 222/300, Loss: 0.0364742189221367, Learning Rate: 0.0001585687205886199\n",
      "Epoch 223/300, Loss: 0.0362754694740229, Learning Rate: 0.0001547744846517318\n",
      "Epoch 224/300, Loss: 0.0349936921951137, Learning Rate: 0.0001510181614135739\n",
      "Epoch 225/300, Loss: 0.03512253334062009, Learning Rate: 0.0001473001627973195\n",
      "Epoch 226/300, Loss: 0.03472835207475892, Learning Rate: 0.0001436208965234148\n",
      "Epoch 227/300, Loss: 0.03465346951933601, Learning Rate: 0.00013998076606486806\n",
      "Epoch 228/300, Loss: 0.03506986440831347, Learning Rate: 0.000136380170603005\n",
      "Epoch 229/300, Loss: 0.033645688685812525, Learning Rate: 0.00013281950498369283\n",
      "Epoch 230/300, Loss: 0.032449127776147446, Learning Rate: 0.0001292991596740415\n",
      "Epoch 231/300, Loss: 0.0351455004862215, Learning Rate: 0.00012581952071958547\n",
      "Epoch 232/300, Loss: 0.033001805903225004, Learning Rate: 0.00012238096970194773\n",
      "Epoch 233/300, Loss: 0.03269862173761748, Learning Rate: 0.00011898388369699624\n",
      "Epoch 234/300, Loss: 0.03461661614194701, Learning Rate: 0.00011562863523349331\n",
      "Epoch 235/300, Loss: 0.032932823028745534, Learning Rate: 0.00011231559225224303\n",
      "Epoch 236/300, Loss: 0.03188783434938781, Learning Rate: 0.00010904511806574305\n",
      "Epoch 237/300, Loss: 0.03306824845980994, Learning Rate: 0.00010581757131834274\n",
      "Epoch 238/300, Loss: 0.031980357380418836, Learning Rate: 0.00010263330594691401\n",
      "Epoch 239/300, Loss: 0.03163623477367661, Learning Rate: 9.949267114203836e-05\n",
      "Epoch 240/300, Loss: 0.03329807595361637, Learning Rate: 9.639601130971379e-05\n",
      "Epoch 241/300, Loss: 0.03394816568286359, Learning Rate: 9.33436660335871e-05\n",
      "Epoch 242/300, Loss: 0.03293707573055467, Learning Rate: 9.033597003771482e-05\n",
      "Epoch 243/300, Loss: 0.03129859914696669, Learning Rate: 8.737325314985631e-05\n",
      "Epoch 244/300, Loss: 0.030127097086250026, Learning Rate: 8.445584026530532e-05\n",
      "Epoch 245/300, Loss: 0.03099623970876012, Learning Rate: 8.158405131126074e-05\n",
      "Epoch 246/300, Loss: 0.030248796331542958, Learning Rate: 7.875820121174347e-05\n",
      "Epoch 247/300, Loss: 0.030930721307102636, Learning Rate: 7.597859985306155e-05\n",
      "Epoch 248/300, Loss: 0.030957440810301638, Learning Rate: 7.324555204982696e-05\n",
      "Epoch 249/300, Loss: 0.029883471400111536, Learning Rate: 7.055935751153021e-05\n",
      "Epoch 250/300, Loss: 0.029739308253496508, Learning Rate: 6.792031080967299e-05\n",
      "Epoch 251/300, Loss: 0.029746611968060083, Learning Rate: 6.532870134546531e-05\n",
      "Epoch 252/300, Loss: 0.030357454325768012, Learning Rate: 6.278481331809015e-05\n",
      "Epoch 253/300, Loss: 0.029220432538209083, Learning Rate: 6.028892569353643e-05\n",
      "Epoch 254/300, Loss: 0.030742701425959792, Learning Rate: 5.784131217400825e-05\n",
      "Epoch 255/300, Loss: 0.029496985593739942, Learning Rate: 5.5442241167910304e-05\n",
      "Epoch 256/300, Loss: 0.0293185518394354, Learning Rate: 5.309197576041338e-05\n",
      "Epoch 257/300, Loss: 0.029958968557700326, Learning Rate: 5.0790773684604345e-05\n",
      "Epoch 258/300, Loss: 0.029039818675646298, Learning Rate: 4.853888729322334e-05\n",
      "Epoch 259/300, Loss: 0.030597051605582237, Learning Rate: 4.633656353098928e-05\n",
      "Epoch 260/300, Loss: 0.029629687814018393, Learning Rate: 4.418404390752093e-05\n",
      "Epoch 261/300, Loss: 0.02903024397224565, Learning Rate: 4.208156447085154e-05\n",
      "Epoch 262/300, Loss: 0.02837598406344275, Learning Rate: 4.002935578154395e-05\n",
      "Epoch 263/300, Loss: 0.029188368740621248, Learning Rate: 3.802764288740764e-05\n",
      "Epoch 264/300, Loss: 0.027784421073296404, Learning Rate: 3.607664529881846e-05\n",
      "Epoch 265/300, Loss: 0.029011489564104924, Learning Rate: 3.4176576964647736e-05\n",
      "Epoch 266/300, Loss: 0.02863664632733864, Learning Rate: 3.2327646248800225e-05\n",
      "Epoch 267/300, Loss: 0.027709259616234636, Learning Rate: 3.053005590736439e-05\n",
      "Epoch 268/300, Loss: 0.028595438038435162, Learning Rate: 2.8784003066378248e-05\n",
      "Epoch 269/300, Loss: 0.02648101591422588, Learning Rate: 2.708967920021202e-05\n",
      "Epoch 270/300, Loss: 0.027567860564287706, Learning Rate: 2.5447270110570818e-05\n",
      "Epoch 271/300, Loss: 0.029164671732843678, Learning Rate: 2.3856955906119732e-05\n",
      "Epoch 272/300, Loss: 0.027857632317309138, Learning Rate: 2.2318910982732418e-05\n",
      "Epoch 273/300, Loss: 0.02652956656168533, Learning Rate: 2.0833304004366946e-05\n",
      "Epoch 274/300, Loss: 0.028508232151971586, Learning Rate: 1.9400297884569785e-05\n",
      "Epoch 275/300, Loss: 0.026916455100231534, Learning Rate: 1.8020049768610386e-05\n",
      "Epoch 276/300, Loss: 0.02664164133086989, Learning Rate: 1.6692711016248786e-05\n",
      "Epoch 277/300, Loss: 0.02618520640874211, Learning Rate: 1.541842718513684e-05\n",
      "Epoch 278/300, Loss: 0.02715225721697641, Learning Rate: 1.4197338014856439e-05\n",
      "Epoch 279/300, Loss: 0.02629093047750147, Learning Rate: 1.3029577411595715e-05\n",
      "Epoch 280/300, Loss: 0.02764832442980024, Learning Rate: 1.1915273433464115e-05\n",
      "Epoch 281/300, Loss: 0.026745490258253072, Learning Rate: 1.0854548276449986e-05\n",
      "Epoch 282/300, Loss: 0.027457846375771717, Learning Rate: 9.847518261020041e-06\n",
      "Epoch 283/300, Loss: 0.02580146159080765, Learning Rate: 8.894293819363682e-06\n",
      "Epoch 284/300, Loss: 0.02789676191780386, Learning Rate: 7.994979483282762e-06\n",
      "Epoch 285/300, Loss: 0.027180397435079648, Learning Rate: 7.14967387272874e-06\n",
      "Epoch 286/300, Loss: 0.026097567740214777, Learning Rate: 6.358469684987329e-06\n",
      "Epoch 287/300, Loss: 0.027030595967286748, Learning Rate: 5.621453684513923e-06\n",
      "Epoch 288/300, Loss: 0.02729605784333205, Learning Rate: 4.938706693418357e-06\n",
      "Epoch 289/300, Loss: 0.027515088243386412, Learning Rate: 4.310303582601976e-06\n",
      "Epoch 290/300, Loss: 0.025677261885868597, Learning Rate: 3.736313263547492e-06\n",
      "Epoch 291/300, Loss: 0.02564870610927479, Learning Rate: 3.2167986807615425e-06\n",
      "Epoch 292/300, Loss: 0.02648732287785675, Learning Rate: 2.7518168048725676e-06\n",
      "Epoch 293/300, Loss: 0.025930519463329374, Learning Rate: 2.3414186263831814e-06\n",
      "Epoch 294/300, Loss: 0.02565417094509813, Learning Rate: 1.9856491500783564e-06\n",
      "Epoch 295/300, Loss: 0.026522248606138592, Learning Rate: 1.6845473900903703e-06\n",
      "Epoch 296/300, Loss: 0.026457894145499303, Learning Rate: 1.4381463656202375e-06\n",
      "Epoch 297/300, Loss: 0.026856680624658548, Learning Rate: 1.2464730973170658e-06\n",
      "Epoch 298/300, Loss: 0.027652217664673358, Learning Rate: 1.109548604314673e-06\n",
      "Epoch 299/300, Loss: 0.025715251677209817, Learning Rate: 1.0273879019266845e-06\n",
      "Epoch 300/300, Loss: 0.026919598089931888, Learning Rate: 1e-06\n",
      "Fold 10 R2: 0.647108793258667\n",
      "\n",
      "Average R2 across 10 folds: 0.618503189086914\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_r2_scores = []\n",
    "\n",
    "dataset = UHIDataset(X_tensor, y_tensor)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "    \n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "    num_epochs = 300\n",
    "    \n",
    "    model = Model(input_dim=X_tensor.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        \n",
    "    model.eval()\n",
    "    val_preds_list = []\n",
    "    val_targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "           \n",
    "            outputs = model(inputs)\n",
    "          \n",
    "            val_preds_list.append(outputs.numpy())\n",
    "            val_targets_list.append(targets.numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds_list, axis=0)\n",
    "    val_targets = np.concatenate(val_targets_list, axis=0)\n",
    "    \n",
    "    val_preds_orig = scaler_y.inverse_transform(val_preds)\n",
    "    val_targets_orig = scaler_y.inverse_transform(val_targets)\n",
    "    \n",
    "    fold_r2 = r2_score(val_targets_orig, val_preds_orig)\n",
    "    print(f\"Fold {fold+1} R2: {fold_r2}\")\n",
    "    fold_r2_scores.append(fold_r2)\n",
    "\n",
    "avg_r2 = np.mean(fold_r2_scores)\n",
    "print(f\"\\nAverage R2 across {k} folds: {avg_r2}\")\n",
    "\n",
    "# num_epochs = 200\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0\n",
    "    \n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     avg_loss = running_loss / len(train_loader)\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     preds_scaled = model(X_tensor)  \n",
    "#     preds_orig = scaler_y.inverse_transform(preds_scaled.numpy())\n",
    "#     y_orig = y.values.reshape(-1, 1)\n",
    "#     loss = criterion(preds_scaled, y_tensor)\n",
    "#     r2 = r2_score(y_orig, preds_orig)\n",
    "#     print(f'Loss: {loss.item()}, R2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "                   'B07', 'B08', 'B8A', 'B11', 'B12', 'LST', 'is_building', \n",
    "                   'Air Temp at Surface [degC]', 'Relative Humidity [percent]', \n",
    "                   'Avg Wind Speed [m/s]', 'Wind Direction [degrees]', 'Solar Flux [W/m^2]']\n",
    "\n",
    "submission = pd.read_csv('validation_imputed.csv')\n",
    "\n",
    "submission_prepared = submission[feature_columns].copy()\n",
    "\n",
    "submission_prepared_scaled = scaler_X.transform(submission_prepared)\n",
    "\n",
    "submission_tensor = torch.tensor(submission_prepared_scaled, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_predictions_tensor = model(submission_tensor)\n",
    "    final_predictions_np = scaler_y.inverse_transform(final_predictions_tensor.numpy())\n",
    "\n",
    "final_prediction_series = pd.Series(final_predictions_np.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('Submission_template.csv')\n",
    "\n",
    "submission_df = pd.DataFrame({'Longitude':sub['Longitude'].values, 'Latitude':sub['Latitude'].values, 'UHI Index':final_prediction_series.values})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
