{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Multi-dimensional arrays and datasets\n",
    "import xarray as xr\n",
    "\n",
    "# Geospatial raster data handling\n",
    "import rioxarray as rxr\n",
    "\n",
    "# Geospatial data analysis\n",
    "import geopandas as gpd\n",
    "\n",
    "# Geospatial operations\n",
    "import rasterio\n",
    "from rasterio import windows  \n",
    "from rasterio import features  \n",
    "from rasterio import warp\n",
    "from rasterio.warp import transform_bounds \n",
    "from rasterio.windows import from_bounds \n",
    "\n",
    "import pyogrio\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# Coordinate transformations\n",
    "from pyproj import Proj, Transformer, CRS\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Feature Importance\n",
    "import shap \n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor \n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor    \n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "# from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# Planetary Computer Tools\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "\n",
    "# Others\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('UHI_data.csv')\n",
    "bronx_df = pd.read_excel('NY_Mesonet_Weather.xlsx', sheet_name='Bronx')\n",
    "manhattan_df = pd.read_excel('NY_Mesonet_Weather.xlsx', sheet_name='Manhattan')\n",
    "submission = pd.read_csv('Validation_with_coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_train_data(df, bronx, manhattan, tolerance=0.01, bronx_lat=40.87248,\n",
    "               bronx_lon=-73.89352, manhattan_lat=40.76754, manhattan_lon=-73.96449):\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "    bronx['Date / Time'] = pd.to_datetime(bronx['Date / Time'])\n",
    "    manhattan['Date / Time'] = pd.to_datetime(manhattan['Date / Time'])\n",
    "\n",
    "    bronx.rename(columns={'Date / Time': 'datetime'}, inplace=True)\n",
    "    manhattan.rename(columns={'Date / Time': 'datetime'}, inplace=True)\n",
    "\n",
    "    # Filter CSV for Manhattan based on coordinate tolerance:\n",
    "    manhattan_csv = df[\n",
    "        (df['Latitude'].between(manhattan_lat - tolerance, manhattan_lat + tolerance)) |\n",
    "        (df['Longitude'].between(manhattan_lon - tolerance, manhattan_lon + tolerance))\n",
    "    ]\n",
    "\n",
    "    bronx_csv = df[\n",
    "        (df['Latitude'].between(bronx_lat - tolerance, bronx_lat + tolerance)) |\n",
    "        (df['Longitude'].between(bronx_lon - tolerance, bronx_lon + tolerance))\n",
    "    ]\n",
    "\n",
    "    print(\"Manhattan CSV rows found:\", len(manhattan_csv))\n",
    "    print(\"Bronx CSV rows found:\", len(bronx_csv))\n",
    "\n",
    "    # Merge the temperature data from Excel with the corresponding CSV data based on datetime.\n",
    "    # For Manhattan:\n",
    "    merged_manhattan = pd.merge(manhattan_csv, manhattan, on='datetime', how='inner')\n",
    "    print(\"Merged Manhattan data:\")\n",
    "    print(merged_manhattan.head())\n",
    "\n",
    "    # For Bronx, if any rows are found; if not, consider a wider tolerance\n",
    "    if not bronx_csv.empty:\n",
    "        merged_bronx = pd.merge(bronx_csv, bronx, on='datetime', how='inner')\n",
    "        print(\"Merged Bronx data:\")\n",
    "        print(merged_bronx.head())\n",
    "    else:\n",
    "        print(\"No Bronx rows found in CSV with tolerance of Â±0.01. Consider increasing tolerance.\")\n",
    "\n",
    "    manhattan_lookup = merged_manhattan[['Longitude', 'Latitude',\n",
    "                                      'Air Temp at Surface [degC]',\n",
    "                                      'Relative Humidity [percent]',\n",
    "                                      'Avg Wind Speed [m/s]',\n",
    "                                      'Wind Direction [degrees]',\n",
    "                                      'Solar Flux [W/m^2]']]\n",
    "\n",
    "    bronx_lookup = merged_bronx[['Longitude', 'Latitude',\n",
    "                                'Air Temp at Surface [degC]',\n",
    "                                'Relative Humidity [percent]',\n",
    "                                'Avg Wind Speed [m/s]',\n",
    "                                'Wind Direction [degrees]',\n",
    "                                'Solar Flux [W/m^2]']]\n",
    "\n",
    "    # Combine (concatenate) the two lookup DataFrames into one.\n",
    "    lookup = pd.concat([manhattan_lookup, bronx_lookup])\n",
    "\n",
    "    # Now merge the main DataFrame (uhi) with the combined lookup DataFrame on the key columns.\n",
    "    df = pd.merge(df, lookup, on=['Longitude', 'Latitude'], how='left')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhi = merge_train_data(df, bronx_df, manhattan_df, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=10, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_cols = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "                'B07', 'B08', 'B8A', 'B11', 'B12']\n",
    "weather_cols = [\n",
    "    'Air Temp at Surface [degC]',\n",
    "    'Relative Humidity [percent]',\n",
    "    'Avg Wind Speed [m/s]',\n",
    "    'Wind Direction [degrees]',\n",
    "    'Solar Flux [W/m^2]'\n",
    "]\n",
    "\n",
    "coord_cols=['Longitude', 'Latitude']\n",
    "\n",
    "impute_cols = coord_cols + band_cols + weather_cols\n",
    "\n",
    "# Check that all required columns exist in uhi.\n",
    "missing_cols = [col for col in impute_cols if col not in uhi.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The following columns are missing from the DataFrame: {missing_cols}\")\n",
    "\n",
    "# Create a copy of the relevant data for imputation.\n",
    "data_to_impute = uhi[impute_cols].copy()\n",
    "\n",
    "# Initialize the KNNImputer.\n",
    "imputer = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "# Fit and transform the data.\n",
    "imputed_array = imputer.fit_transform(data_to_impute)\n",
    "\n",
    "# The imputed_array has the same order as impute_cols.\n",
    "# Extract the imputed weather data columns. They are at the end of the array.\n",
    "weather_start_idx = len(coord_cols) + len(band_cols)\n",
    "imputed_weather = imputed_array[:, weather_start_idx:]\n",
    "\n",
    "uhi[weather_cols] = imputed_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in weather_cols:\n",
    "    if col not in submission.columns:\n",
    "        submission[col] = np.nan\n",
    "\n",
    "test_for_impute = submission[impute_cols].copy()\n",
    "\n",
    "# Use the trained imputer to transform the test data.\n",
    "imputed_array = imputer.transform(test_for_impute)\n",
    "\n",
    "# The imputer was trained on data with columns in the order:\n",
    "#   coord_cols + band_cols + weather_cols.\n",
    "# Extract the imputed weather columns (the last len(weather_cols) columns).\n",
    "imputed_weather = imputed_array[:, -len(weather_cols):]\n",
    "\n",
    "# Update the test DataFrame's weather columns with the imputed values.\n",
    "submission[weather_cols] = imputed_weather\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y), and then into training and testing sets\n",
    "X = uhi.drop(columns=['Longitude','Latitude','datetime','UHI Index']).values\n",
    "y = uhi['UHI Index'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_xg(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "#         # 'tree_method': trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist'])\n",
    "#     }\n",
    "    \n",
    "#     model = XGBRegressor(**params, random_state=42, tree_method='exact')\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study_xg = optuna.create_study(direction='maximize')  \n",
    "# study_xg.optimize(objective_xg, n_trials=100)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study_xg.best_params)\n",
    "# print('Best Score:', study_xg.best_value)\n",
    "\n",
    "# best_params_xg = study_xg.best_params\n",
    "\n",
    "# with open(\"xg_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_xg.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_xg.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_lgbm(trial):\n",
    "#     # params = {\n",
    "#     #     'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1),\n",
    "#     #     'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#     #     'num_leaves': trial.suggest_int('num_leaves', 10, 150),\n",
    "#     #     'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#     #     'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#     #     'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "#     #     'bagging_freq': trial.suggest_int('bagging_freq', 3, 7),\n",
    "#     #     'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),  \n",
    "#     #     'lambda_l2': trial.suggest_float('lambda_l2', 0, 10),\n",
    "#     #     'n_estimators': trial.suggest_int('n_estimators', 100, 1000)  \n",
    "#     # }\n",
    "#     params = {\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-6, 0.5),      \n",
    "#         'max_depth': trial.suggest_int('max_depth', 2, 50),                   \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 5, 300),                 \n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 200),       \n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),   \n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),               \n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 0, 50),                    \n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 0, 50),\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 1500)              \n",
    "#     }\n",
    "\n",
    "\n",
    "#     model = LGBMRegressor(**params, random_state=42, verbose=-1)\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "\n",
    "#     return score\n",
    "\n",
    "# study_lgbm = optuna.create_study(direction='maximize')\n",
    "# study_lgbm.optimize(objective_lgbm, n_trials=1000)\n",
    "\n",
    "# print('Best Hyperparameters:', study_lgbm.best_params)\n",
    "# print('Best Score:', study_lgbm.best_value)\n",
    "\n",
    "# best_params_lgbm = study_lgbm.best_params\n",
    "\n",
    "# with open(\"lgbm_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_lgbm.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_lgbm.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_rf(trial):\n",
    "#     params = {\n",
    "#         'n_estimators' : trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "#         'max_depth' : trial.suggest_int(\"max_depth\", 5, 30),\n",
    "#         'min_samples_split' : trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "#         'min_samples_leaf' : trial.suggest_int(\"min_samples_leaf\", 2, 20),\n",
    "#         'max_features' : trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "#     }\n",
    "    \n",
    "#     model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study_rf = optuna.create_study(direction='maximize')  \n",
    "# study_rf.optimize(objective_rf, n_trials=100)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study_rf.best_params)\n",
    "# print('Best Score:', study_rf.best_value)\n",
    "\n",
    "# best_params_rf = study_rf.best_params\n",
    "\n",
    "# with open(\"rf_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_rf.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_rf.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_cat(trial):\n",
    "#     params = {\n",
    "#         \"iterations\": trial.suggest_int(\"iterations\", 500, 2000, step=100),\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "#         \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "#         \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-5, 10, log=True),\n",
    "#         \"random_strength\": trial.suggest_float(\"random_strength\", 0, 10),\n",
    "#         \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1),\n",
    "#         \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "#         \"loss_function\": \"RMSE\",\n",
    "#         \"eval_metric\": \"RMSE\",\n",
    "#         \"random_seed\": 42,\n",
    "#         \"verbose\": 0\n",
    "#     }\n",
    "    \n",
    "#     model = CatBoostRegressor(**params)\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study_cat = optuna.create_study(direction='maximize')  \n",
    "# study_cat.optimize(objective_cat, n_trials=100)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study_cat.best_params)\n",
    "# print('Best Score:', study_cat.best_value)\n",
    "\n",
    "# best_params_cat = study_cat.best_params\n",
    "\n",
    "# with open(\"cat_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_cat.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_cat.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_gbr(trial):\n",
    "#     params = {\n",
    "#         #'loss':trial.suggest_categorical('loss', ['squared_error', 'huber', 'quantile', 'absolute_error']),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.0),\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "#         'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "#         #'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
    "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#     }\n",
    "    \n",
    "#     model = GradientBoostingRegressor(**params, random_state=42, criterion='squared_error')\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study_gbr = optuna.create_study(direction='maximize')  \n",
    "# study_gbr.optimize(objective_gbr, n_trials=100)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study_gbr.best_params)\n",
    "# print('Best Score:', study_gbr.best_value)\n",
    "\n",
    "# best_params_gbr = study_gbr.best_params\n",
    "\n",
    "# with open(\"gbr_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_gbr.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_gbr.best_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_hgbr(trial):\n",
    "#     params = {\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.0),\n",
    "#         'max_iter': trial.suggest_int('max_iter', 100, 2000),\n",
    "#         'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 200),   \n",
    "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'l2_regularization': trial.suggest_loguniform('l2_regularization', 1e-5, 1.0),\n",
    "#         'max_bins': trial.suggest_int('max_bins', 100, 255),\n",
    "#     }\n",
    "    \n",
    "#     model = HistGradientBoostingRegressor(**params, random_state=42, loss='squared_error')\n",
    "#     # model.fit(X_train, y_train)\n",
    "\n",
    "#     # preds = model.predict(X_test)\n",
    "#     # score = r2_score(y_test, preds)\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "#     score = scores.mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study_hgbr = optuna.create_study(direction='maximize')  \n",
    "# study_hgbr.optimize(objective_hgbr, n_trials=100)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study_hgbr.best_params)\n",
    "# print('Best Score:', study_hgbr.best_value)\n",
    "\n",
    "# best_params_hgbr = study_hgbr.best_params\n",
    "\n",
    "# with open(\"hgbr_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Best Score: {study_hgbr.best_value}\\n\")\n",
    "#     f.write(f\"Best Hyperparameters: {study_hgbr.best_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xg = {'n_estimators': 527, 'learning_rate': 0.0312788331848764, 'max_depth': 17, 'subsample': 0.748120538929842, 'reg_alpha': 0.001241366749156092, 'reg_lambda': 4.027151002258514}\n",
    "best_params_lgbm = {'learning_rate': 0.049306462164681195, 'max_depth': 12, 'num_leaves': 145, 'min_data_in_leaf': 8, 'feature_fraction': 0.8758609827849215, 'bagging_fraction': 0.24707848190077422, 'bagging_freq': 0, 'lambda_l1': 0.0029913663437004133, 'lambda_l2': 6.191482291982284, 'n_estimators': 1318}\n",
    "best_params_rf = {'n_estimators': 713, 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(**best_params_xg, random_state=42, tree_method='exact')\n",
    "# model = LGBMRegressor(**best_params_lgbm, random_state=42, verbose=-1)\n",
    "# model = CatBoostRegressor(**best_params_cat, verbose=False)\n",
    "# model = GradientBoostingRegressor(**best_params_gbr, random_state=42, criterion='squared_error')\n",
    "# model = HistGradientBoostingRegressor(**best_params_hgbr, random_state=42, loss='squared_error')\n",
    "# model = RandomForestRegressor(**best_params_rf, random_state=42, n_jobs=-1)\n",
    "# estimators = [\n",
    "#     ('xgb', XGBRegressor(**best_params_xg, random_state=42, tree_method='exact')),\n",
    "    # ('cat', CatBoostRegressor(**best_params_cat, verbose=False)),\n",
    "    # ('lgb', LGBMRegressor(**best_params_lgbm, random_state=42, verbose=-1)),\n",
    "    # ('gbr', GradientBoostingRegressor(**best_params_gbr, random_state=42, criterion='squared_error')),\n",
    "    # ('hgbr', HistGradientBoostingRegressor(**best_params_hgbr, random_state=42, loss='squared_error'))\n",
    "# ]\n",
    "# model = StackingRegressor(\n",
    "#     estimators=estimators, \n",
    "#     final_estimator=RandomForestRegressor(**best_params_rf, random_state=42, n_jobs=-1),\n",
    "    # final_estimator=XGBRegressor(**best_params_xg, random_state=42, tree_method='exact'),\n",
    "    # final_estimator=GradientBoostingRegressor(**best_params_gbr, random_state=42, criterion='squared_error'),\n",
    "#     cv=10,\n",
    "#     passthrough=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_predictions = model.predict(X_train)\n",
    "Y_train = y_train.tolist()\n",
    "r2_score(Y_train, insample_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_predictions = model.predict(X_test)\n",
    "Y_test = y_test.tolist()\n",
    "r2_score(Y_test, outsample_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "                   'B07', 'B08', 'B8A', 'B11', 'B12', 'LST', 'is_building', \n",
    "                   'Air Temp at Surface [degC]', 'Relative Humidity [percent]', \n",
    "                   'Avg Wind Speed [m/s]', 'Wind Direction [degrees]', 'Solar Flux [W/m^2]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, if submission originally has more columns, select only the ones used in training:\n",
    "submission_prepared = submission[feature_columns].copy()\n",
    "\n",
    "# Now predict:\n",
    "final_predictions = model.predict(submission_prepared)\n",
    "\n",
    "final_prediction_series = pd.Series(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('Submission_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'Longitude':sub['Longitude'].values, 'Latitude':sub['Latitude'].values, 'UHI Index':final_prediction_series.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
